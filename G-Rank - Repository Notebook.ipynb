{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a933bea",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8071827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib import rc\n",
    "import random\n",
    "\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "\n",
    "\n",
    "# Output destinations, set these to whateve you want\n",
    "datasets_loc = '~/Documents/Delft/Thesis/LTR/Results/FINAL/Datasets/'\n",
    "figures_loc = '~/Documents/Delft/Thesis/LTR/Results/FINAL/Figures/'\n",
    "\n",
    "# Load source, don't alter\n",
    "repository_loc = 'https://github.com/awrgold/G-Rank/blob/main/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"song\", \"songID\", \"artist\", \"album\", \"genre\", \"play_count\", \"click_count\"]\n",
    "\n",
    "df_lib = pd.read_csv(repository_loc + 'df_lib_tags_v2.csv?raw=true', index_col=0)\n",
    "# tags = pd.read_csv(repository_loc + 'tags.csv?raw=true', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.Series([\n",
    "    'acoustic',\n",
    "    'alternative',\n",
    "    'ambient',\n",
    "    'blues',\n",
    "    'classical',\n",
    "    'comedy',\n",
    "    'country',\n",
    "    'dance',\n",
    "    'dream_pop',\n",
    "    'drum_n_bass',\n",
    "    'dubstep',\n",
    "    'electronic',\n",
    "    'experimental',\n",
    "    'folk',\n",
    "    'funk',\n",
    "    'garage',\n",
    "    'hardcore',\n",
    "    'hip_hop',\n",
    "    'house',\n",
    "    'idm',\n",
    "    'indie',\n",
    "    'industrial',\n",
    "    'jazz',\n",
    "    'metal',\n",
    "    'noise',\n",
    "    'pop',\n",
    "    'post_rock',\n",
    "    'progressive_rock',\n",
    "    'psychedelic',\n",
    "    'punk',\n",
    "    'reggae',\n",
    "    'rock',\n",
    "    'shoegaze',\n",
    "    'ska',\n",
    "    'soul',\n",
    "    'techno',\n",
    "    'trance',\n",
    "    'trip_hop',\n",
    "    'world_music',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65311fb7",
   "metadata": {},
   "source": [
    "---\n",
    "# NODE OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    nodeID = None\n",
    "    library = None\n",
    "    clicklog = None\n",
    "    gossip_progress = None\n",
    "    queryID = 0\n",
    "    known_nodes = None\n",
    "    round_infected = 999999999 # Python doesn't have INT.MAX_VALUE as Int is unbound\n",
    "    my_data = None\n",
    "    accepting = True\n",
    "    \n",
    "    def __init__(self, newID, network_size):\n",
    "        self.nodeID = newID\n",
    "        self.library = pd.DataFrame()\n",
    "        self.clicklog = pd.DataFrame(columns=['key',\n",
    "                                              'nodeID',\n",
    "                                              'query_term',\n",
    "                                              'seen_before',\n",
    "                                              'results_order',\n",
    "                                              'item_clicked',\n",
    "                                              'item_clicked_title',\n",
    "                                              'item_clicked_tags'\n",
    "                                             ])\n",
    "        self.known_nodes = pd.DataFrame(index=range(network_size), columns=['similarity', 'gossip_progress'])\n",
    "        self.known_nodes['gossip_progress'] = 0\n",
    "        self.update_local()\n",
    "\n",
    "    \n",
    "    def update_library(self, to_append, single_item):\n",
    "        \n",
    "        if (single_item):\n",
    "                    \n",
    "            # Make sure they're the same shape\n",
    "            if (list(to_append.keys()) == list(self.library.columns.values)):\n",
    "\n",
    "                self.library = pd.concat([self.library, to_append], ignore_index=False).drop_duplicates()\n",
    "                self.library = self.library.loc[self.library.astype(str).drop_duplicates().index]\n",
    "            \n",
    "                \n",
    "        else:\n",
    "            self.library = pd.concat([self.library, to_append], ignore_index=False)\n",
    "            self.library = self.library.loc[self.library.astype(str).drop_duplicates().index]        \n",
    "    \n",
    "    \n",
    "    def update_clicklog(self, to_append, single_item, update, propagate):\n",
    "        \n",
    "        if(single_item):\n",
    "\n",
    "            # Make sure they're the same shape\n",
    "            if (list(to_append.keys()) == list(self.clicklog.columns.values)):\n",
    "                self.clicklog = self.clicklog.append(to_append, ignore_index=True)\n",
    "                self.clicklog.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                if (update):\n",
    "                    # Update only the values of the incoming nodes\n",
    "                    self.update_known_nodes(to_append.nodeID.unique())\n",
    "\n",
    "\n",
    "        # If we're appending multiple items\n",
    "        else:\n",
    "            \n",
    "            self.clicklog = pd.concat([self.clicklog, to_append], ignore_index=True)\n",
    "            self.clicklog = self.clicklog.loc[self.clicklog.astype(str).drop_duplicates().index]\n",
    "            self.clicklog.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            if (update):\n",
    "                # Update only the values of the incoming nodes\n",
    "                self.update_known_nodes(to_append.nodeID.unique())\n",
    "                \n",
    "        # propagate to one other node\n",
    "        if (propagate):\n",
    "            \n",
    "            # Neighbors are nodes that have S > 0.0\n",
    "            neighbors = self.known_nodes.iloc[np.where(self.known_nodes.similarity > 0.0)].index.values\n",
    "            new_target = np.random.choice(neighbors)\n",
    "            \n",
    "            if (network.nodes[new_target].accepting):\n",
    "                network.nodes[new_target].update_clicklog(to_append, single_item, update, propagate=False)\n",
    "\n",
    "        self.update_local()\n",
    "\n",
    "            \n",
    "            \n",
    "    # Give a target node and gossip the clicklog\n",
    "    def gossip(self, target, to_gossip, sample_rate, update, propagate):\n",
    "        \n",
    "        \n",
    "        # if sample_rate = 0.0, gossip 1 random item\n",
    "        if (sample_rate == 0.0):\n",
    "            \n",
    "            gossip = to_gossip.loc[np.random.choice(to_gossip.index)]\n",
    "            \n",
    "        elif (sample_rate < 1.0):\n",
    "            \n",
    "            # Use np.random.choice to avoid .sample() which creates a new dataframe object\n",
    "            random_indices = np.random.choice(to_gossip.index.values, size=(int)(sample_rate * len(to_gossip)), replace=False)\n",
    "            gossip = to_gossip.loc[random_indices]\n",
    "            \n",
    "        # Otherwise gossip the whole clicklog\n",
    "        else: \n",
    "            gossip = to_gossip            \n",
    "            \n",
    "        if (target.accepting):\n",
    "            target.update_clicklog(gossip, single_item=False, update=update, propagate=propagate)\n",
    "            \n",
    "        if (update):\n",
    "            to_update = list(to_gossip.nodeID.unique()).append(target.nodeID)\n",
    "#             self.update_known_nodes([to_update])\n",
    "            self.update_known_nodes([target.nodeID])\n",
    "        \n",
    "    \n",
    "    def update_known_nodes(self, to_update):\n",
    "        \n",
    "        # If to_update is None, update them all\n",
    "        if (to_update is None) or all(x is None for x in to_update):\n",
    "            \n",
    "            # Calculate similarity\n",
    "            self.known_nodes['similarity'] = pd.Series([jaccard_similarity(self, x) for x in network.nodes])\n",
    "            \n",
    "            # Standardize the results\n",
    "            if np.max(self.known_nodes['similarity']) != 0:\n",
    "                self.known_nodes['similarity'] = self.known_nodes['similarity'] / np.max(self.known_nodes['similarity'])\n",
    "        \n",
    "        else:\n",
    "            # Update the list of known nodes and their distances to the list of to_update nodes\n",
    "            for peer in to_update:\n",
    "                self.known_nodes['similarity'][peer] = jaccard_similarity(self, network.nodes[peer])\n",
    "\n",
    "            # Standardize the results\n",
    "            if np.max(self.known_nodes['similarity']) != 0:\n",
    "                self.known_nodes['similarity'] = self.known_nodes['similarity'] / np.max(self.known_nodes['similarity'])\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def update_local(self):\n",
    "\n",
    "        self.my_data = self.clicklog.iloc[np.where(self.clicklog.nodeID.to_numpy() == self.nodeID)]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e86cb",
   "metadata": {},
   "source": [
    "# NETWORK OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2af8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    nodes = None\n",
    "    default_cols = None    \n",
    "    library = None\n",
    "    clicklog = None\n",
    "    node_distances = None\n",
    "    home_nodes = None\n",
    "   \n",
    "   \n",
    "            \n",
    "    def __init__(self, num_nodes, default_df, default_log, fraction, network_size):\n",
    "        \n",
    "        self.library = default_df\n",
    "        self.nodes = pd.Series()\n",
    "        self.clicklog = default_log\n",
    "        self.node_distances = pd.DataFrame()\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "\n",
    "            newNode = Node(i, network_size)\n",
    "            \n",
    "            # fraction is the % of the global dataset each node will contain (randomly selected)\n",
    "            random_indices = np.random.choice(self.library.index.values, size=(int)(fraction * len(self.library)), replace=False)\n",
    "            \n",
    "            newNode.library = self.library.iloc[random_indices]\n",
    "            newNode.clicklog = self.clicklog\n",
    "            self.nodes = pd.concat([self.nodes, pd.Series([newNode])], ignore_index=True)\n",
    "        \n",
    "        \n",
    "    def add_node(self, newNode, fraction):\n",
    "\n",
    "        random_indices = np.random.choice(self.library.index.values, size=(int)(fraction * len(self.library)), replace=False)\n",
    "        newNode.library = self.library.iloc[random_indices]\n",
    "        self.nodes = pd.concat([self.nodes, pd.Series([newNode])], ignore_index=True)\n",
    "\n",
    "    def update_clicklog(self, to_append, single_item):\n",
    "        \n",
    "        if(single_item):\n",
    "            # Make sure they're the same shape\n",
    "            if (list(to_append.keys()) == list(self.clicklog.columns.values)):\n",
    "                self.clicklog = self.clicklog.append(to_append, ignore_index=True)\n",
    "        else:\n",
    "            if (list(to_append.keys()) == list(self.clicklog.columns.values)):\n",
    "                self.clicklog = pd.concat([self.clicklog, to_append], ignore_index=True).drop_duplicates().reset_index(inplace=False, drop=True)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c045e",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "\n",
    "Compute the (modified) Jaccard similarity between nodes. If two nodes have issued the same query and clicked on the same result, they will have a score  `S > 0.0`.\n",
    "\n",
    "Jaccard Similarity between two node clicklogs in its most basic form is:\n",
    "\n",
    "$$ S_{i}(n_j) = \\frac{|List(A) \\cap List(B)|}{|List(A) \\cup List(B)|} $$\n",
    "\n",
    "Where $S_{i}(n_j)$ is the similarity score $S$ assigned to node $n_j$ for node $n_i$.\n",
    "\n",
    "\n",
    "The modified version takes into consideration not just the intersection of list items, but multiple intersections. See Thesis report.\n",
    "\n",
    "\n",
    "- Find the cardinality of the intersection of the top $K$ query terms $T^{K}(Q)$ between $n_i$ and $n_j$, denoted as $\\kappa_{t}$.\n",
    "- Find the square of the cardinality of the intersection of clicked results for all query terms $C_i(Q)$ and $C_j(Q)$ between $n_i$ and $n_j$, denoted as $\\kappa_{m}$.\n",
    "- The sum $\\kappa_{t} + (\\kappa_{m})^{2}$ is divided by the cardinality of the the union of clicked results for all query terms $C_i(Q)$ and $C_j(Q)$ between $n_i$ and $n_j$, denoted as $\\kappa_{u}$.\n",
    "\n",
    "$$ S_{i}(n_j) = \\frac{\\kappa_{t} + (\\kappa_{m})^{2}}{\\kappa_{u}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard_similarity(my_node, their_node):\n",
    "    \n",
    "    \n",
    "    # Our searches\n",
    "    my_terms = my_node.my_data.query_term.values\n",
    "    my_clicks = my_node.my_data.item_clicked.values\n",
    "\n",
    "    \n",
    "    # The other known node's searches\n",
    "    their_data = my_node.clicklog.iloc[np.where(my_node.clicklog.nodeID.to_numpy() == their_node.nodeID)]\n",
    "    their_terms = their_data.query_term.values\n",
    "    their_clicks = their_data.item_clicked.values\n",
    "\n",
    "    # The terms both of us have searched for\n",
    "    intersecting_terms = np.intersect1d(my_terms, their_terms)\n",
    "            \n",
    "    # Get our top 10 query terms\n",
    "    my_top = my_node.clicklog.iloc[np.where(my_node.clicklog.nodeID.to_numpy() == my_node.nodeID)].groupby('query_term')['query_term'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(10)\n",
    "    their_top = my_node.clicklog.iloc[np.where(my_node.clicklog.nodeID.to_numpy() == their_node.nodeID)].groupby('query_term')['query_term'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(10)\n",
    "\n",
    "    top_intersection = np.intersect1d(my_top.query_term, their_top.query_term)\n",
    "\n",
    "    top_score = len(top_intersection)\n",
    "        \n",
    "    # if an intersection of search terms exists\n",
    "    if len(intersecting_terms) > 0:\n",
    "\n",
    "        # All terms in my clicklog that is also in theirs\n",
    "        containing_terms = np.where([my_node.clicklog.query_term.to_numpy() == x for x in intersecting_terms])\n",
    "\n",
    "        my_locs = np.where(my_node.clicklog.nodeID.to_numpy() == my_node.nodeID)\n",
    "        their_locs = np.where(my_node.clicklog.nodeID.to_numpy() == their_node.nodeID)\n",
    "\n",
    "        # Find all instances of my clicks that are in this intersection\n",
    "        my_intersecting_clicks = my_node.clicklog.iloc[\n",
    "            np.intersect1d(\n",
    "                my_locs,\n",
    "                containing_terms\n",
    "            )]\n",
    "\n",
    "        # Find all instances of their clicks that are in this intersection\n",
    "        their_intersecting_clicks = my_node.clicklog.iloc[\n",
    "            np.intersect1d(\n",
    "                their_locs,\n",
    "                containing_terms\n",
    "            )]\n",
    "\n",
    "        # Get the intersection of both of these clicks, \n",
    "        # i.e. where both nodes searched for the same query and clicked on the same item\n",
    "        if (len(my_intersecting_clicks) > 0 and len(their_intersecting_clicks) > 0):\n",
    "            true_intersection = np.intersect1d(my_intersecting_clicks.item_clicked, their_intersecting_clicks.item_clicked)\n",
    "            \n",
    "        else:\n",
    "            true_intersection = []\n",
    "        \n",
    "        union = np.union1d(my_clicks, their_clicks)\n",
    "        score = (len(true_intersection) + (top_score**2)) / len(union)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        score = 0.0\n",
    "        \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012119f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas imports lists of strings as a string of a list of strings. This method extracts and separates them.\n",
    "def extract_tags(tags_string):\n",
    "    \n",
    "    extracted = tags_string.split('\\'')\n",
    "    one = extracted[1]\n",
    "    three = extracted[3]\n",
    "    five = extracted[5]\n",
    "    \n",
    "    tags = [one, three, five]\n",
    "    \n",
    "    return tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference in index locations for intersecting values between two lists\n",
    "\n",
    "def list_distance(true_rank, node_rank):\n",
    "    \n",
    "    true_rank, node_rank = pd.Series(true_rank), pd.Series(node_rank)\n",
    "    \n",
    "    nums = np.intersect1d(true_rank.to_numpy(), node_rank.to_numpy())\n",
    "        \n",
    "    # return the sum of positional edits for one list to become another list\n",
    "    return sum([np.abs(node_rank.loc[node_rank==x].index.values[0] - true_rank.loc[true_rank==x].index.values[0]) for x in nums])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f21cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c017cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# Search_ranked\n",
    "\n",
    "\n",
    "\n",
    "Node $N_i$ searches for a query term $Q$:\n",
    "1.  If $Q$ seen in `N.clicklog.query_term` before:\n",
    "    1.  Find all instances that $N_i$ has searched for $Q$\n",
    "        1.  Create a list $L_{N_i}(C_Q)$ of clicked items $C_Q$ for node $N_i$ and query $Q$, sorted by number of clicks per item\n",
    "    1.  Find all instances that other nodes $j = \\{0, ..., N\\}$ have searched for $Q$\n",
    "        1. Create a list $L_{N_j}(C_Q)$ of clicked items $C_Q$ for all nodes $N_j$ and query $Q$\n",
    "            1. Sort this list by weighting results by `SIMILARITY` \n",
    "    1. Append these two lists, with $L_{N_i}(C_Q)$ above $L_{N_j}(C_Q)$ for all $N_j$\n",
    "    1. Flip a coin:\n",
    "        1. If heads, randomly swap the positions of 2 items in the list\n",
    "    1. Return $L$\n",
    " \n",
    "1. If $Q$ is not in `N.clicklog`:\n",
    "    1. Check if this term is contained in any local metadata:\n",
    "        1. Create a list $L_{N_i}(M_Q)$ of local items containing direct metadata matches $M_Q$\n",
    "    1. Check the clicklog if $Q$ is contained in `N.clicklog.clicked_item_tags`, and append song matches to above list\n",
    "    1. If no local metadata and no matches in clicklog:\n",
    "        1. Return the most popular items in `N.clicklog`, sorted by number of clicks\n",
    "            \n",
    "            \n",
    "\n",
    "---\n",
    "       \n",
    "`SIMILARITY` can be found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Search RANKED\n",
    "# Uses distances of known nodes to rank results\n",
    "# Node then asks neighbors for library items for each result that matches the query term, if it doesn't have it already\n",
    "# Afterwards, it appends items \n",
    "# \n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "def search_ranked_v2(query_term, node, network, normalization):\n",
    "\n",
    "    query_term = str(query_term)\n",
    "    query_term = query_term.lower()\n",
    "    query_term = re.sub(r'\\W+ ', '', query_term)\n",
    "    query_term = sys.intern(query_term)\n",
    "\n",
    "\n",
    "    ### (1A) ###\n",
    "#     past_query_matches = node.clicklog.iloc[np.where(node.clicklog['query_term'].astype(str).str.contains(query_term))].dropna(axis=0, inplace=False)\n",
    "    past_query_matches = node.clicklog.iloc[np.where(node.clicklog['query_term'].to_numpy() == query_term)].dropna(axis=0, inplace=False)\n",
    "    \n",
    "    if(DEBUG):\n",
    "        print('-------')\n",
    "        print('Node:', node.nodeID, '\\nquery_term:', query_term)\n",
    "        print('past_query_matches:\\n', past_query_matches)\n",
    "        print('-------')\n",
    "\n",
    "\n",
    "    # If this specific query has been seen before\n",
    "    if not past_query_matches.empty:\n",
    "        \n",
    "        seen_before = True\n",
    "\n",
    "\n",
    "        #####\n",
    "        # Order clicklog results based on local node's distances to known_nodes['similarity']\n",
    "        # Sort the clicklog results by the sorted values of the known node distances\n",
    "        # Return a list of clicklog results for the specific query term\n",
    "        #####\n",
    "        \n",
    "        \n",
    "        # If the node is aware of more nodes than just itself\n",
    "        if (len(node.clicklog.nodeID.unique()) > 1):            \n",
    "\n",
    "            # get the past CLICKLOG search results that THIS NODE has performed of this query_term\n",
    "            this_node_results = past_query_matches.iloc[np.where(past_query_matches.nodeID.values == node.nodeID)].dropna(axis=0, inplace=False)\n",
    "            other_node_results = pd.concat([past_query_matches.iloc[np.where(past_query_matches.nodeID.values == x.nodeID)] for x in network.nodes if x.nodeID != node.nodeID])\n",
    "\n",
    "            # Sort THIS NODE's results by the number of clicks on each item for this query\n",
    "            sorted_this_node_results = this_node_results.groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "            \n",
    "            # Create a distance column taken from THIS NODE's known_nodes list (going to be 1.0 for all of these)\n",
    "            sorted_this_node_results['similarity'] = pd.Series(node.known_nodes['similarity'].iloc[node.nodeID])\n",
    "            \n",
    "            # Create a score of (distance + 1) * click count (add 1 to distance to ensure that clicks are counted for distances = 0)\n",
    "            sorted_this_node_results['score'] = (sorted_this_node_results['similarity'].to_numpy() + normalization) * sorted_this_node_results['count'].to_numpy()\n",
    "\n",
    "            \n",
    "            # Sort OTHER NODE results by the number of clicks on each item for this query\n",
    "            sorted_other_node_results = other_node_results.groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "            \n",
    "            # Create a distance column taken from THIS NODE's known_nodes list\n",
    "            sorted_other_node_results['similarity'] = pd.Series(node.known_nodes['similarity'].iloc[other_node_results.nodeID.unique()])\n",
    "            \n",
    "            # Create a score of (distance + 1) * click count (add 1 to distance to ensure that clicks are counted for distances = 0)\n",
    "            sorted_other_node_results['score'] = (sorted_other_node_results['similarity'].to_numpy() + normalization) * sorted_other_node_results['count'].to_numpy()\n",
    "\n",
    "            # All items clicked on by both this node and other nodes\n",
    "            all_clicked_items = pd.concat([sorted_this_node_results, sorted_other_node_results]).drop_duplicates(inplace=False, ignore_index=True)\n",
    "\n",
    "            # calculate a score for each of these items based on the similarity score\n",
    "            all_clicked_items['score'] = all_clicked_items['count'].to_numpy() * all_clicked_items['similarity'].to_numpy()\n",
    "            \n",
    "            # rank them by cumulative score\n",
    "            ranked_results = all_clicked_items.groupby('item_clicked')['score'].sum().reset_index(name='score').sort_values(['score'], ascending=False)\n",
    "\n",
    "            # Drop duplicates into a unique list\n",
    "            sorted_items_clicked = ranked_results.astype(str).drop_duplicates(subset='item_clicked', inplace=False)\n",
    "\n",
    "            # Library items of items clicked, sorted by the above rankings\n",
    "            sorted_results = network.library.iloc[sorted_items_clicked.item_clicked.values]\n",
    "\n",
    "            if(DEBUG):\n",
    "                print('len(sorted_results)', len(ranked_results))\n",
    "\n",
    "\n",
    "            if not (ranked_results.empty):\n",
    "                if(DEBUG):\n",
    "                    print('*** RESULTS:\\n', ranked_results.index.values)\n",
    "                    \n",
    "                    \n",
    "            # Add a bit of random noise, occasionally swapping two rows\n",
    "            # I DONT KNOW A MORE PYTHONIC WAY TO DO THIS (without creating copies or using .sample())\n",
    "            if (len(sorted_results) > 1) and (np.random.rand() <= 0.5):\n",
    "                new_index = []\n",
    "                to_swap = np.random.choice(range(len(sorted_results)), size=2, replace=False)\n",
    "                j = 0\n",
    "                for i in range(len(sorted_results.index)):\n",
    "                    if i not in to_swap:\n",
    "                        new_index.append(i)\n",
    "                    else:\n",
    "                        new_index.append(to_swap[j])\n",
    "                        j = j + 1\n",
    "\n",
    "                sorted_results = sorted_results.iloc[new_index]\n",
    "                    \n",
    "            \n",
    "                    \n",
    "            return sorted_results, seen_before\n",
    "                    \n",
    "        else:\n",
    "            \n",
    "            if(DEBUG):\n",
    "                print('node.known_nodes is empty')\n",
    "            \n",
    "            #####\n",
    "            # THIS NODE'S CLICK RESULTS\n",
    "            #####\n",
    "\n",
    "            # get the past CLICKLOG search results that THIS NODE has performed of this query_term\n",
    "            this_node_results = past_query_matches.iloc[np.where(node.nodeID == past_query_matches.nodeID.values)].dropna(axis=0, inplace=False)\n",
    "\n",
    "            # Sort THIS NODE's results by the number of clicks on each item for this query\n",
    "            sorted_node_results = this_node_results.groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "\n",
    "            # Turn the sorted NODE results into a library DF from our local results\n",
    "            top_node_items = node.library.loc[sorted_node_results.item_clicked]\n",
    "\n",
    "            if not this_node_results.empty:\n",
    "                \n",
    "                if(DEBUG):\n",
    "                    print('*** THIS NODE CLICKED ON:\\n', top_node_items.index.values)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            #####\n",
    "            # OTHER NODE CLICK RESULTS\n",
    "            #####\n",
    "\n",
    "            # list from OUR CLICKLOG that OTHER NODES have clicked on for this same query\n",
    "            other_node_results = past_query_matches.iloc[np.where(node.nodeID != past_query_matches.nodeID.values)].dropna(axis=0, inplace=False)\n",
    "            other_nodes = other_node_results.nodeID.values\n",
    "\n",
    "\n",
    "            ## Turn the sorted OTHER NODE results into a library DF ONLY FROM OTHER NODE LIBRARY\n",
    "            other_node_hits = pd.DataFrame()\n",
    "\n",
    "            if (len(other_node_results) > 0):\n",
    "                for item in other_node_results.item_clicked.values:\n",
    "                    for other in other_nodes:\n",
    "                        if item in network.nodes[other].library.index.values:\n",
    "                            other_node_hits = other_node_hits.append(network.nodes[other].library.loc[item], ignore_index=False)\n",
    "\n",
    "\n",
    "            if(DEBUG):\n",
    "                print('*** OTHER NODES THAT HAVE SEEN query_term ' + str(query_term) + ':', other_node_results.nodeID.values)\n",
    "                for each in other_node_results.nodeID.values:\n",
    "                    print('*** NODE: ' + str(each) + ' CLICKED ON:', other_node_hits.index.values)\n",
    "\n",
    "\n",
    "            #####\n",
    "            # LOCAL LIBRARY TAG MATCHES\n",
    "            #####\n",
    "\n",
    "        #     local_library_results = node.library.iloc[np.where(node.library['tags'].astype(str).str.contains(query_term, na=False, case=False))]\n",
    "            local_library_results = node.library.loc[node.library.tags.index[np.where([query_term in str(extract_tags(x)) for x in node.library.tags])[0]]]\n",
    "\n",
    "\n",
    "\n",
    "            #####\n",
    "            # All combined results\n",
    "            #####\n",
    "\n",
    "            # (1) Previous local node clicks (2) Previous other node clicks\n",
    "            results = pd.concat([top_node_items, other_node_hits], ignore_index=False).drop_duplicates()\n",
    "\n",
    "            # (3) other local node library tag matches\n",
    "            results = pd.concat([results, local_library_results], ignore_index=False).drop_duplicates()\n",
    "        #         results = results.iloc[results.astype(str).drop_duplicates().index]\n",
    "\n",
    "            # (4) finish off by adding 10 random local items\n",
    "            results = pd.concat([results, node.library.loc[np.random.choice(node.library.index.values, size=10, replace=False)]]).drop_duplicates()\n",
    "            \n",
    "            if(DEBUG):\n",
    "                print('results', results)\n",
    "\n",
    "\n",
    "            return results, seen_before\n",
    "\n",
    "\n",
    "\n",
    "    # This query has NOT been seen before, let's check for local string matches\n",
    "    else:\n",
    "\n",
    "        seen_before = False\n",
    "\n",
    "        if(DEBUG):\n",
    "            print('Never seen this query before, returning LOCAL STRING MATCHES')\n",
    "\n",
    "        # Get top-N local results with a tag match\n",
    "        results = node.library.iloc[np.where([query_term in x for x in node.library.album.values])[0]]\n",
    "        results = pd.concat([results, node.library.iloc[np.where([query_term in x for x in node.library.artist.values])[0]]])\n",
    "        results = pd.concat([results, node.library.iloc[np.where([query_term in x for x in node.library.tags.values])[0]]])\n",
    "\n",
    "        results = results.loc[results.astype(str).drop_duplicates().index]\n",
    "        \n",
    "        \n",
    "        # If no local string matches:\n",
    "        \n",
    "        if (len(results) == 0):\n",
    "            \n",
    "            if(DEBUG):\n",
    "                print('No local string matches, returning random results')\n",
    "            \n",
    "            results = pd.concat([results, network.library.loc[np.random.choice(network.library.index.values, size=10, replace=False)]])\n",
    "\n",
    "        \n",
    "        return results, seen_before\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88e7a7",
   "metadata": {},
   "source": [
    "---\n",
    "# Choose top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Choose the top item of the results\n",
    "#####\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "def choose_top(results, seen_before, query_term, searching_node, network):\n",
    "\n",
    "    # get the top result\n",
    "    final_result = results.head(1)            \n",
    "\n",
    "\n",
    "    # Create the clicklog entry\n",
    "    clicklog_entry = {\n",
    "                      'key': [searching_node.nodeID, searching_node.queryID],\n",
    "                      'nodeID': searching_node.nodeID, \n",
    "                      'query_term': query_term, \n",
    "                      'seen_before': seen_before,\n",
    "                      'results_order': list(results.index.values), \n",
    "                      'item_clicked': final_result.index.values[0], \n",
    "                      'item_clicked_title': final_result.album.values[0], \n",
    "                      'item_clicked_tags': final_result.tags.values[0]                     \n",
    "                     }\n",
    "\n",
    "    searching_node.queryID = searching_node.queryID + 1\n",
    "\n",
    "    if(DEBUG): \n",
    "        print('-------')\n",
    "        print('clicklog_entry = \\n', clicklog_entry)\n",
    "        print('final_result = \\n', final_result)\n",
    "        print('-------')\n",
    "\n",
    "\n",
    "    # add the query and the results to the clicklog\n",
    "    searching_node.update_library(final_result, True)\n",
    "    searching_node.update_clicklog(clicklog_entry, True, False, False)\n",
    "    network.update_clicklog(clicklog_entry, True)\n",
    "\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406cf8c",
   "metadata": {},
   "source": [
    "## Choose Tiebreaker by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed99800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Choose the LOWEST ID of a tag-match tie\n",
    "#####\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "def choose_tie(results, seen_before, query_term, searching_node, network):\n",
    "    \n",
    "    \n",
    "    if(DEBUG):\n",
    "        print('QUERY TERM:', query_term)  \n",
    "        print('CHOOSE_TIE:\\n', results)\n",
    "\n",
    "    query_term = sys.intern(query_term)\n",
    "    \n",
    "    # If the query_term has been searched for previously by this node and there are more than one options to choose from\n",
    "    if (query_term in searching_node.clicklog.query_term.values) and (len(results) > 1):\n",
    "\n",
    "        # get the results that have tag matches\n",
    "        to_choose = results.iloc[np.where([query_term in str(extract_tags(x)) for x in results.tags])[0]]\n",
    "        \n",
    "        \n",
    "        # Choose the lowest-ID version\n",
    "        lowest = to_choose.iloc[np.where([query_term in str(extract_tags(x)) for x in to_choose.tags])[0]].sort_index()\n",
    "        \n",
    "        if(DEBUG):\n",
    "            print('to_choose', to_choose)\n",
    "            print('lowest', lowest)\n",
    "        \n",
    "        final_result = lowest.head(1)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # Otherwise there's just one result\n",
    "        final_result = results.head(1)\n",
    "\n",
    "\n",
    "        \n",
    "    if(DEBUG):            \n",
    "        print('final_result', final_result)\n",
    "\n",
    "    # Create the clicklog entry\n",
    "    clicklog_entry = {\n",
    "                      'key': [searching_node.nodeID, searching_node.queryID],\n",
    "                      'nodeID': searching_node.nodeID, \n",
    "                      'query_term': sys.intern(query_term), \n",
    "                      'seen_before': seen_before,\n",
    "                      'results_order': list(results.index.values), \n",
    "                      'item_clicked': final_result.index.values[0], \n",
    "                      'item_clicked_title': final_result.album.values[0], \n",
    "                      'item_clicked_tags': final_result.tags.values[0]                     \n",
    "                     }\n",
    "\n",
    "    searching_node.queryID = searching_node.queryID + 1\n",
    "\n",
    "    if(DEBUG): \n",
    "        print('-------')\n",
    "        print('final_result = \\n', final_result)\n",
    "        print('clicklog_entry = \\n', clicklog_entry)\n",
    "        print('-------')\n",
    "\n",
    "\n",
    "    # add the query and the results to the clicklog\n",
    "    searching_node.update_library(final_result, True)\n",
    "    searching_node.update_clicklog(clicklog_entry, True, False, False)\n",
    "    network.update_clicklog(clicklog_entry, True)\n",
    "\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4051e2",
   "metadata": {},
   "source": [
    "---\n",
    "# Choose random\n",
    "\n",
    "Chooses a random result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cfd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Chooses a random result from the list\n",
    "#####\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "def choose_random(results, seen_before, query_term, searching_node, network):\n",
    "    \n",
    "\n",
    "    if (len(results) > 1):\n",
    "        \n",
    "        final_result = results.loc[np.random.choice(results.index.values, size=1)]\n",
    "        \n",
    "    else:\n",
    "        final_result = results.head(1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Create the clicklog entry\n",
    "    clicklog_entry = {\n",
    "                      'key': [searching_node.nodeID, searching_node.queryID],\n",
    "                      'nodeID': searching_node.nodeID, \n",
    "                      'query_term': sys.intern(query_term),\n",
    "                      'seen_before': seen_before,\n",
    "                      'results_order': list(results.index.values), \n",
    "                      'item_clicked': final_result.index.values[0], \n",
    "                      'item_clicked_title': final_result.album.values[0],\n",
    "                      'item_clicked_tags': final_result.tags.values[0]                     \n",
    "                     }\n",
    "\n",
    "    searching_node.queryID = searching_node.queryID + 1\n",
    "\n",
    "    if(DEBUG): \n",
    "        print('-------')\n",
    "        print('clicklog_entry = \\n', clicklog_entry)\n",
    "        print('final_result = \\n', final_result)\n",
    "        print('-------')\n",
    "\n",
    "\n",
    "    # add the query and the results to the clicklog\n",
    "    searching_node.update_library(final_result, True)\n",
    "    searching_node.update_clicklog(clicklog_entry, True, False, False)\n",
    "    network.update_clicklog(clicklog_entry, True)\n",
    "\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac0e2a",
   "metadata": {},
   "source": [
    "# Choose bottom\n",
    "\n",
    "Choose the bottom result, particularly for use by spam nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Chooses the bottom result of the list\n",
    "#################\n",
    "\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "def choose_bottom(results, seen_before, query_term, searching_node, network):\n",
    "\n",
    "    final_result = results.tail(1)\n",
    "\n",
    "    # Create the clicklog entry\n",
    "    clicklog_entry = {\n",
    "                      'key': [searching_node.nodeID, searching_node.queryID],\n",
    "                      'nodeID': searching_node.nodeID, \n",
    "                      'query_term': sys.intern(query_term),\n",
    "                      'seen_before': seen_before,\n",
    "                      'results_order': list(results.index.values), \n",
    "                      'item_clicked': final_result.index.values[0], \n",
    "                      'item_clicked_title': final_result.album.values[0],\n",
    "                      'item_clicked_tags': final_result.tags.values[0]                     \n",
    "                     }\n",
    "\n",
    "    searching_node.queryID = searching_node.queryID + 1\n",
    "\n",
    "    if(DEBUG): \n",
    "        print('-------')\n",
    "        print('clicklog_entry = \\n', clicklog_entry)\n",
    "        print('final_result = \\n', final_result)\n",
    "        print('-------')\n",
    "\n",
    "\n",
    "    # add the query and the results to the clicklog\n",
    "    searching_node.update_library(final_result, True)\n",
    "    searching_node.update_clicklog(clicklog_entry, True, False, False)\n",
    "    network.update_clicklog(clicklog_entry, True)\n",
    "\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9415cd",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b112904",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9d5c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06cdcb",
   "metadata": {},
   "source": [
    "# Simulation Parameters and Structures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed619fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "\n",
    "# Number of benign nodes at start of simulation\n",
    "num_nodes = 100\n",
    "\n",
    "# Number of searches in the simulation\n",
    "num_searches = 10000\n",
    "\n",
    "# Print statements\n",
    "DEBUG = False\n",
    "DEBUG2 = False\n",
    "PRINT_DEBUG = True\n",
    "\n",
    "# Testing time/profiling messages\n",
    "TEST = True\n",
    "\n",
    "# Measuring metrics and evaluation of the simulation - turn off to speed up/stop measuring regularly\n",
    "MEASURING = True\n",
    "\n",
    "# The number of searches/queries between each measurement round\n",
    "MEASURE_RATE = 100\n",
    "last_measure_time = datetime.datetime.now() - datetime.datetime.now() # instantiate as a datetime.delta\n",
    "\n",
    "# Set tick = 0 in case you need to reset the simulation\n",
    "tick = 0\n",
    "\n",
    "\n",
    "# Comment any combination of these out to reduce the number of simulations performed\n",
    "simulations = [\n",
    "#     '_baseline_',\n",
    "#     '_clicklog_inflation_',\n",
    "    '_targeted_sybil_',\n",
    "#     '_epic_sybil_',\n",
    "#     '_push_'\n",
    "]\n",
    "\n",
    "# Same here, comment whatever\n",
    "isolation_params = [\n",
    "    0.0,\n",
    "#     1.0\n",
    "]\n",
    "\n",
    "\n",
    "pull = True\n",
    "\n",
    "\n",
    "# When do attackers begin?\n",
    "join_time = (int)(num_searches / 4)\n",
    "attack_time = (int)(num_searches / 2)\n",
    "   \n",
    "    \n",
    "# The number of searches/clicklog entries each attacker will attempt to share with the network per attack round\n",
    "num_spam = 100\n",
    "\n",
    "    \n",
    "# [0.0, 1.0] Percentage of clicklog entries that a newly bootstrapped node will be given \n",
    "bootstrap_rate = 0.1\n",
    "\n",
    "# Set to true to save all simulation output automatically, otherwise manually run the save cell further below\n",
    "AUTOSAVE = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd8e07",
   "metadata": {},
   "source": [
    "# Simulation Loop\n",
    "\n",
    "\n",
    "This loop will loop through EVERY combination of simulation, initializing a fresh network. It will perform a simulation, save the results, and then reset and perform the next one.\n",
    "\n",
    "Comment out above simulation parameters to constrain the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1408529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_start = datetime.datetime.now()\n",
    "\n",
    "for f in isolation_params:\n",
    "    \n",
    "    if f == 0.0:\n",
    "        F = '_F0_'\n",
    "        F_title = 'F = 0'\n",
    "        isolation = 0.0\n",
    "\n",
    "    elif f == 1.0:\n",
    "        F = '_F1_'\n",
    "        F_title = 'F = 1'\n",
    "        isolation = 1.0\n",
    "        \n",
    "    \n",
    "    for sim in simulations:\n",
    "        \n",
    "        sim_start_time = datetime.datetime.now()\n",
    "        \n",
    "        print('--------------------------------------------------------------------------')\n",
    "        print('New Simulation: ' + str(f) + ' - ' + sim + ' - Starting at: ', sim_start_time)\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        \n",
    "        baseline = False\n",
    "        clicklog_inflation = False\n",
    "        targeted_sybil = False\n",
    "        epic_attack = False\n",
    "        push = False\n",
    "        \n",
    "\n",
    "        if sim == '_baseline_':\n",
    "            \n",
    "            model = '_baseline_'\n",
    "            model_title = 'Baseline'\n",
    "            baseline = True\n",
    "            num_attackers = 0\n",
    "            total_network_size = num_nodes + num_attackers\n",
    "\n",
    "        elif sim == '_clicklog_inflation_':\n",
    "            \n",
    "            model = '_clicklog_inflation_'\n",
    "            model_title = 'Clicklog Inflation Attack'\n",
    "            clicklog_inflation = True\n",
    "            num_attackers = (int)(num_nodes/10)\n",
    "            total_network_size = num_nodes + num_attackers\n",
    "            num_spam = 1000\n",
    "\n",
    "        elif sim == '_targeted_sybil_':\n",
    "            \n",
    "            model = '_targ_sybil_'\n",
    "            model_title = 'Targeted Sybil Attack'\n",
    "            targeted_sybil = True\n",
    "            num_attackers = (int)(num_nodes/10)\n",
    "            total_network_size = num_nodes + num_attackers\n",
    "            num_spam = 100\n",
    "        \n",
    "\n",
    "        elif sim == '_epic_sybil_':\n",
    "            \n",
    "            model = '_epic_sybil_'\n",
    "            model_title = 'Epic Sybil Attack'\n",
    "            epic_sybil = True\n",
    "            num_attackers = (int)(num_nodes * 3)\n",
    "            total_network_size = num_nodes + num_attackers\n",
    "            num_spam = 100\n",
    "\n",
    "        elif sim == '_push_':\n",
    "        \n",
    "            model = '_push_'\n",
    "            model_title = 'Push vs. Pull Comparison'\n",
    "            mod = '_Push_'\n",
    "            modifier = 'Push Gossip Scheme'\n",
    "            push = True\n",
    "            pull = False\n",
    "            num_attackers = (int)(num_nodes/10)\n",
    "            total_network_size = num_nodes + num_attackers\n",
    "            num_spam = 100\n",
    "\n",
    "        if pull:\n",
    "            modifier = 'Pull Gossip Scheme'\n",
    "            \n",
    "        which_sim = str(model_title + ' - ' + modifier + ' - ' + F_title)\n",
    "    \n",
    "        print(which_sim)\n",
    "        print('Number of nodes:', num_nodes)\n",
    "        print('Number of attackers:', num_attackers)\n",
    "        print('Total network size:', total_network_size)\n",
    "\n",
    "        \n",
    "        \n",
    "################################################## INITIALIZE NETWORK ################################################\n",
    "        \n",
    "        ### Create the basic network example\n",
    "        clicklog = pd.DataFrame(columns=['key', 'nodeID', 'query_term', 'seen_before', 'results_order', 'item_clicked', 'item_clicked_title', 'item_clicked_tags'])\n",
    "        dataset = df_lib\n",
    "\n",
    "        network = Network(num_nodes, dataset, clicklog, fraction=0.1, network_size = total_network_size)\n",
    "        tick = 0\n",
    "\n",
    "        # Create the pre-existing local node clicklogs including all previous node results + 1 new one\n",
    "        for node in network.nodes:\n",
    "\n",
    "            print('Initializing node:', node.nodeID)\n",
    "\n",
    "            # If you want some of the nodes to be unaccepting of unsolicited gossip\n",
    "            if node.nodeID % 2 == 0:\n",
    "                node.accepting = False\n",
    "\n",
    "\n",
    "\n",
    "            for query_term in tags:\n",
    "\n",
    "                # Get all the library items that match the tag\n",
    "                matches = network.library.iloc[np.where(network.library['tags'].astype(str).str.contains(query_term, na=False, case=False))]\n",
    "\n",
    "                # Pick a random item to click on\n",
    "                item_clicked = matches.loc[np.random.choice(matches.index.values, size=1)]\n",
    "                item_clicked_index = item_clicked.index.values[0]\n",
    "                item_clicked_title = item_clicked.album.values[0]\n",
    "                item_clicked_tags = item_clicked.tags.values[0]\n",
    "\n",
    "                seen_before = False\n",
    "\n",
    "                # Add the song to the user's library\n",
    "                node.update_library(item_clicked, True)\n",
    "\n",
    "\n",
    "                clicklog_entry = {\n",
    "                                  'key': [node.nodeID, node.queryID],\n",
    "                                  'nodeID': node.nodeID, \n",
    "                                  'query_term': sys.intern(query_term),\n",
    "                                  'seen_before': seen_before,\n",
    "                                  'results_order': list(matches.index.values), \n",
    "                                  'item_clicked': item_clicked_index,\n",
    "                                  'item_clicked_title': item_clicked_title,\n",
    "                                  'item_clicked_tags': item_clicked.tags.values[0]\n",
    "                                 }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Update the clicklog\n",
    "                node.update_clicklog(clicklog_entry, single_item=True, update=False, propagate=False)\n",
    "                network.update_clicklog(clicklog_entry, single_item=True)\n",
    "                node.queryID = node.queryID + 1\n",
    "                node.update_known_nodes([node.nodeID])\n",
    "\n",
    "\n",
    "            if (node.nodeID > 0):\n",
    "\n",
    "                # Choose a random node to bootstrap the current node (must have already been initialized)\n",
    "                bootstrap_index = np.random.choice(range(node.nodeID), size=1)\n",
    "                bootstrap_node = network.nodes[bootstrap_index].values[0]\n",
    "\n",
    "                bootstrap_clicklog_indices = np.random.choice(range(len(bootstrap_node.clicklog)), size=(int)(np.ceil(bootstrap_rate * len(bootstrap_node.clicklog))))\n",
    "                bootstrap_clicklog = bootstrap_node.clicklog.iloc[bootstrap_clicklog_indices]\n",
    "\n",
    "                # bootstrap with a subset of the previous node's clicklog\n",
    "                node.update_clicklog(bootstrap_clicklog, single_item=False, update=False, propagate=False)\n",
    "                node.update_known_nodes([bootstrap_node.nodeID])\n",
    "\n",
    "        # Choose a random node to gossip to the first node (as all other nodes have received gossip except the first)\n",
    "        bootstrap_index = np.random.choice(range(len(network.nodes)), size=1)\n",
    "        bootstrap_node = network.nodes[bootstrap_index].values[0]\n",
    "\n",
    "        bootstrap_clicklog_indices = np.random.choice(range(len(bootstrap_node.clicklog)), size=(int)(np.ceil(bootstrap_rate * len(bootstrap_node.clicklog))))\n",
    "        bootstrap_clicklog = bootstrap_node.clicklog.iloc[bootstrap_clicklog_indices]\n",
    "\n",
    "        # bootstrap with a subset of the previous node's clicklog\n",
    "        network.nodes[0].update_clicklog(bootstrap_clicklog, single_item=False, update=False, propagate=False)\n",
    "        network.nodes[0].update_known_nodes([bootstrap_node.nodeID])\n",
    "\n",
    "\n",
    "        \n",
    "################################################### INITIAL MEASUREMENTS ####################################################\n",
    "        \n",
    "        \n",
    "        tag_popularity = pd.DataFrame(tags, columns = ['tag'])\n",
    "\n",
    "        term_metrics = pd.DataFrame(columns = ['mean', 'median', 'min', 'max', 'first_q', 'last_q'])\n",
    "        node_metrics = pd.DataFrame(columns = ['mean', 'median', 'min', 'max', 'first_q', 'last_q'])\n",
    "\n",
    "\n",
    "        medians = []\n",
    "        log_size = []\n",
    "\n",
    "        term_distances_over_time = []\n",
    "        node_distances_over_time = []\n",
    "\n",
    "        term_avg_rankings_over_time = []\n",
    "        node_avg_rankings_over_time = []\n",
    "\n",
    "        term_median_rankings_over_time = []\n",
    "        node_median_rankings_over_time = []\n",
    "\n",
    "        measurement_times = []\n",
    "        \n",
    "        gossip_message_size = []\n",
    "        avg_gossip_message_size = []\n",
    "\n",
    "\n",
    "        infected_nodes = []\n",
    "        infected_targets = pd.Series(index=[range(0,(int)(num_searches / MEASURE_RATE))])\n",
    "        \n",
    "        \n",
    "        # every MEASURE_RATE we evaluate the simulation, including at the very beginning\n",
    "        if (tick == 0) and (MEASURING):\n",
    "\n",
    "            start_time = datetime.datetime.now()\n",
    "            measure_start = datetime.datetime.now()\n",
    "            print(measure_start)\n",
    "\n",
    "            clicklog_sizes = []\n",
    "\n",
    "            for n in network.nodes:\n",
    "                clicklog_sizes.append(len(n.clicklog))\n",
    "\n",
    "            # Keep track of the round, the median, and mean of the size of the clicklog\n",
    "            log_size.append([tick, np.median(clicklog_sizes), np.mean(clicklog_sizes)])\n",
    "\n",
    "\n",
    "\n",
    "            # Only measure metrics from benign nodes (to get a pure global snapshot)\n",
    "            benign_clicklog = network.clicklog.iloc[np.where(network.clicklog.nodeID.to_numpy() < num_nodes)]\n",
    "\n",
    "            ########################\n",
    "            # Keep track of the most popular song per tag\n",
    "            print('Checking nodes containing most popular song')\n",
    "            print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "            best_song = []\n",
    "            popularity = []\n",
    "\n",
    "\n",
    "            for tag in tags:\n",
    "                # Most popular song by amount of clicks (global) PER TAG\n",
    "                most_pop_song = benign_clicklog.iloc[np.where(benign_clicklog.query_term.to_numpy() == tag)].groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(1).item_clicked.values[0]\n",
    "                best_song.append(most_pop_song)\n",
    "\n",
    "            tag_popularity['most_popular_song_round_' + str(tick)] = best_song\n",
    "\n",
    "\n",
    "            ##########################\n",
    "            # Keep track of the nodes having seen the most popular song per tag\n",
    "            for pop_song_id in tag_popularity['most_popular_song_round_' + str(tick)].values:\n",
    "\n",
    "                present = 0\n",
    "\n",
    "                # Only looking at benevolent nodes\n",
    "                for node in network.nodes[:num_nodes]:\n",
    "                    if pop_song_id in node.library.index.values:\n",
    "                        present = present + 1\n",
    "\n",
    "                popularity.append(present)\n",
    "\n",
    "\n",
    "            # The median number of nodes containing the most popular songs\n",
    "            # We want to see how this grows as the number of queries grows\n",
    "            medians.append(np.median(popularity) / num_nodes)\n",
    "\n",
    "            print('Adding to Tag Popularity Dataframe')\n",
    "            print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "            tag_popularity['num_nodes_containing_round_' + str((int)(tick / MEASURE_RATE))] = popularity\n",
    "\n",
    "\n",
    "\n",
    "            ###############################\n",
    "            # GLOBAL:\n",
    "            # Change in average and median distance between true rankings and local rankings\n",
    "            #####\n",
    "\n",
    "\n",
    "            print('Calculating best global ranking for each term')\n",
    "            print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "\n",
    "            # The pairwise Jaccard similarity for all benevolent nodes (0:num_nodes)\n",
    "            global_node_distances = pd.DataFrame([[jaccard_similarity(node, network.nodes[x]) for x in range(0, num_nodes)] for node in network.nodes])\n",
    "\n",
    "\n",
    "            # get the best rankings for each term based on the number of clicks\n",
    "            best_rankings = pd.DataFrame(columns=tags)\n",
    "\n",
    "\n",
    "            for tag in tags.values:\n",
    "\n",
    "                # All songs containing the tag in its metadata\n",
    "                all_songs_with_tag = df_lib.iloc[np.where([tag in str(extract_tags(x)) for x in df_lib.tags.values])]\n",
    "\n",
    "                # All global clicklog results for this tag\n",
    "                tag_results = benign_clicklog.iloc[np.where(benign_clicklog['query_term'].to_numpy() == tag)].dropna(axis=0, inplace=False)\n",
    "\n",
    "                # Sorted tag results by number of clicks\n",
    "                ranking = tag_results.groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "\n",
    "                # All songs with the tag not in the clicklog yet\n",
    "                other_songs = [other for other in all_songs_with_tag.index.values if other not in ranking.item_clicked.values]\n",
    "\n",
    "                # \"Proper\" ranking based on number of clicks across all nodes in the network (including zero clicks)\n",
    "                proper_ranking = ranking.item_clicked.append(pd.Series(other_songs), ignore_index=True)\n",
    "\n",
    "                # Dataset containing all global rankings for this tag\n",
    "                best_rankings[tag] = [proper_ranking.values]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #########################\n",
    "            # LOCAL:\n",
    "            # Get the local rankings for all nodes and terms\n",
    "\n",
    "            print('Calculating the best local ranking for all terms at each node')\n",
    "            print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "            node_rankings_per_tag = pd.DataFrame(columns=tags)\n",
    "\n",
    "            # for all nodes (including malicious nodes)\n",
    "            for local_node in network.nodes:\n",
    "\n",
    "                node_best_rankings = pd.DataFrame(columns=tags)\n",
    "\n",
    "                # Same as global, but only on local clicklog data\n",
    "                for term in tags.values:\n",
    "\n",
    "                    # All songs containing the tag in its metadata\n",
    "                    node_all_songs_with_term = df_lib.iloc[np.where([term in str(extract_tags(x)) for x in df_lib.tags.values])]\n",
    "\n",
    "                    # All local clicklog results for this tag\n",
    "                    node_term_results = local_node.clicklog.iloc[np.where(local_node.clicklog['query_term'].to_numpy() == term)].dropna(axis=0, inplace=False)\n",
    "\n",
    "                    # Ranking for this tag according to this node's clicklog\n",
    "                    node_ranking = node_term_results.groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "\n",
    "                    # Songs in the node's library that have not been searched/clicked on yet\n",
    "                    node_other_songs = [local for local in node_all_songs_with_term.index.values if local not in node_ranking.item_clicked.values]\n",
    "\n",
    "                    # \"Proper\" ranking based on local data\n",
    "                    node_proper_ranking = node_ranking.item_clicked.append(pd.Series(node_other_songs), ignore_index=True)\n",
    "\n",
    "                    # Dataset containing the most recent ranking per tag at this time\n",
    "                    node_best_rankings[term] = pd.Series([node_proper_ranking.values])\n",
    "\n",
    "                # Keep track of all best rankings per tag, per node\n",
    "                node_rankings_per_tag = pd.concat([node_rankings_per_tag, node_best_rankings], ignore_index=True)\n",
    "\n",
    "\n",
    "            # Get the total distances between the optimal ranking and local ranking for all terms\n",
    "            print('Calculating total distances between optimal ranking and local ranking for all terms')\n",
    "            print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "            # The distance between the best ranking for TERM and node's ranking for TERM, for all nodes, for all terms (including malicious nodes)\n",
    "            distances_term = pd.Series([[list_distance(best_rankings[term][0], node_rankings_per_tag[term].iloc[y]) for y in range(len(network.nodes))] for term in tags])\n",
    "\n",
    "            # Distance between best ranking for each term and the node's ranking for the term, across all terms, for each node (including malicious nodes)\n",
    "            distances_node = pd.Series([[list_distance(best_rankings[term][0], node_rankings_per_tag[term].iloc[y]) for term in tags] for y in range(len(network.nodes))])\n",
    "\n",
    "\n",
    "            node_distances_over_time.append(distances_node.values)\n",
    "            term_distances_over_time.append(distances_term.values)\n",
    "\n",
    "\n",
    "\n",
    "            ################## Look for infected nodes ##################\n",
    "            if(not baseline):\n",
    "                for node in network.nodes:\n",
    "                    if np.max(node.clicklog.nodeID) >= num_nodes:\n",
    "\n",
    "                        # \"Round\" meaning each measurement round\n",
    "                        node.round_infected = np.min([node.round_infected, (int)(tick / MEASURE_RATE)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            measure_end = datetime.datetime.now()\n",
    "            measure_time = measure_end - measure_start\n",
    "            measurement_times.append(measure_time)\n",
    "\n",
    "            if (PRINT_DEBUG):\n",
    "\n",
    "                print('--------------------------------------------------------')\n",
    "                print('Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "                print('Measurement round time:' + str(measure_time))\n",
    "                print('--------------------------------------------------------')\n",
    "\n",
    "            last_measure_time = measure_time\n",
    "\n",
    "\n",
    "            \n",
    "############################################# PERFORM SIMULATION LOOP ##################################################\n",
    "\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        print('STARTING AT: ', str(start_time))\n",
    "\n",
    "        for tick in range(0, num_searches + 1):\n",
    "\n",
    "\n",
    "            if(PRINT_DEBUG):\n",
    "                print('---------------------------------------------------------------------------')\n",
    "                print('--- TICK #' + str(tick))\n",
    "\n",
    "\n",
    "        ############################################## QUERIES ###################################################\n",
    "\n",
    "            # Perform query simulation\n",
    "            query_start = datetime.datetime.now()\n",
    "\n",
    "            # Define a random tag to search the network for\n",
    "            random_tag = str(tags.iloc[np.random.randint(0, len(tags))])\n",
    "\n",
    "            # casts everything to alphanumeric + spaces\n",
    "            random_tag = re.sub(r'\\W+ ', '', random_tag)\n",
    "            tag_end = datetime.datetime.now()\n",
    "\n",
    "\n",
    "            # A random node will initiate the search (avoid selecting malicious nodes pre-attack)\n",
    "            if (tick < attack_time):\n",
    "                random_node = network.nodes[np.random.choice(range(num_nodes))]\n",
    "            else:\n",
    "                random_node = network.nodes[np.random.choice(range(len(network.nodes)))]\n",
    "\n",
    "            # Force the node to update its known nodes before searching and gossiping\n",
    "#             random_node.update_known_nodes(random_node.clicklog.nodeID.unique())\n",
    "            update_end = datetime.datetime.now()\n",
    "\n",
    "            # assign the results\n",
    "            search_results, seen_before = search_ranked_v2(random_tag, random_node, network, isolation)\n",
    "            search_end = datetime.datetime.now()\n",
    "\n",
    "\n",
    "            # choose the item to click on\n",
    "            clicked_item = choose_tie(search_results, seen_before, random_tag, random_node, network)\n",
    "            click_end = datetime.datetime.now()\n",
    "\n",
    "            if (TEST):\n",
    "                print('Tag time:', tag_end - query_start)\n",
    "                print('Updating ' + str(len(random_node.clicklog.nodeID.unique())) + ' known nodes time:', update_end - tag_end)\n",
    "                print('Search time:', search_end - update_end)\n",
    "                print('Click time:', click_end - search_end)\n",
    "\n",
    "\n",
    "            # Pull means nodes request gossip from nodes they are aware of\n",
    "            if (pull):\n",
    "\n",
    "                neighbor_end = datetime.datetime.now()\n",
    "\n",
    "                # choose a potential target to request gossip from, cannot be self and must have a similarity score (somehow have interacted before)\n",
    "                targets = random_node.known_nodes.index.values[np.where(random_node.known_nodes.index.values != random_node.nodeID) and np.where(random_node.known_nodes.similarity >= 0)]\n",
    "                targets = network.nodes[np.random.choice(targets, size=1, replace=False)]\n",
    "#                 targets = network.nodes[targets]\n",
    "                \n",
    "                for target in targets:\n",
    "\n",
    "                    # get all clicklog entries that this node hasn't yet already shared with you\n",
    "                    target_progress = target.known_nodes['gossip_progress'].iloc[random_node.nodeID]\n",
    "                    to_receive = target.clicklog.iloc[target_progress:]\n",
    "\n",
    "                    # We want recipient of gossip to update known nodes\n",
    "                    target.gossip(random_node, to_receive, sample_rate = 1.0, update=True, propagate=False)\n",
    "                    gossip_message_size.append(to_receive.memory_usage().sum())\n",
    "                    \n",
    "                    target.known_nodes['gossip_progress'].iloc[random_node.nodeID] = target.clicklog.tail(1).index.values[0]\n",
    "                    gossip_end = datetime.datetime.now()\n",
    "\n",
    "                    if (TEST): \n",
    "                        print('Calculating neighbors time:', neighbor_end - click_end)\n",
    "                        print('Gossip time:', gossip_end - neighbor_end)\n",
    "\n",
    "                    if (PRINT_DEBUG):\n",
    "                        print('Node ' + str(random_node.nodeID) + '\\'s requested gossip from:\\n', [target.nodeID])\n",
    "                        if(np.max(to_receive.nodeID.values) > num_nodes):\n",
    "                            print('*********************************** INFECTED CLICKLOG GOSSIP RECEIVED!')\n",
    "\n",
    "            # Otherwise gossip is outbound and unrequested (push)\n",
    "            elif (push):\n",
    "\n",
    "                \n",
    "                # If the node is aware of more nodes than just itself\n",
    "                if (len(random_node.clicklog.nodeID.unique()) > 2):\n",
    "\n",
    "                    # List of gossip options that aren't itself\n",
    "                    options = [x for x in random_node.clicklog.nodeID.unique() if x != random_node.nodeID]\n",
    "\n",
    "                    # Choose exactly 2 targets (unless it's benign, then choose one)\n",
    "                    gossip_targets = network.nodes[np.random.choice(options, size=1 if random_node.nodeID < num_nodes else 2, replace=False)]\n",
    "\n",
    "                # If it doesn't know 2 unique nodes besides itself:\n",
    "                elif (len(random_node.clicklog.nodeID.unique()) == 2):\n",
    "                    \n",
    "                    # List of gossip options that aren't itself\n",
    "                    options = [x for x in random_node.clicklog.nodeID.unique() if x != random_node.nodeID]\n",
    "\n",
    "                    # Choose exactly one target\n",
    "                    gossip_targets = network.nodes[np.random.choice(options, size=1, replace=False)]\n",
    "                \n",
    "                # If it doesn't know any unique nodes to gossip to (can't happen with bootstrap, but edge case)\n",
    "                else:\n",
    "                    gossip_targets = []\n",
    "                    \n",
    "                \n",
    "                # This will just be one target anyways (as benign nodes)\n",
    "                for target in gossip_targets:\n",
    "\n",
    "                    # Find out what the gossip progress is for the target node\n",
    "                    target_progress = random_node.known_nodes['gossip_progress'].iloc[target.nodeID]\n",
    "\n",
    "                    # Only gossip clicklog entries that you haven't shared to this target node yet\n",
    "                    to_gossip = random_node.clicklog.iloc[target_progress:]\n",
    "\n",
    "                    # We want update=True because then we don't need to do a full update_known_nodes\n",
    "                    gossip_start = datetime.datetime.now()\n",
    "                    \n",
    "                    random_node.gossip(target, to_gossip, sample_rate = 1.0, update=True, propagate=False)\n",
    "                    gossip_message_size.append(to_gossip.memory_usage().sum())\n",
    "                    \n",
    "                    gossip_end = datetime.datetime.now()\n",
    "\n",
    "                    # update the gossip progress for the target node\n",
    "                    random_node.known_nodes['gossip_progress'].iloc[target.nodeID] = random_node.clicklog.tail(1).index.values[0]\n",
    "\n",
    "                    if (TEST): \n",
    "                        print('Gossip update=True time:', gossip_end - gossip_start)\n",
    "\n",
    "                if (PRINT_DEBUG):\n",
    "                    print('Node ' + str(random_node.nodeID) + '\\'s gossip targets:\\n', [target.nodeID for target in gossip_targets])\n",
    "                    if(np.max(to_gossip.nodeID.values) > num_nodes):\n",
    "                            print('*********************************** INFECTED CLICKLOG GOSSIP SENT!')\n",
    "\n",
    "            query_end = datetime.datetime.now()\n",
    "\n",
    "            if(PRINT_DEBUG):\n",
    "                print('Query Loop Time:', (query_end - query_start))\n",
    "                print('--------------------------------------------------------')\n",
    "\n",
    "\n",
    "########################################################## ATTACKS ##########################################################\n",
    "\n",
    "\n",
    "            # If it's time for malicious nodes to join\n",
    "            if (not baseline) and (tick == join_time):\n",
    "\n",
    "                # Bootstrap all malicious nodes\n",
    "                print('BOOTSTRAP PHASE: Malicious nodes being bootstrapped.')\n",
    "                print(model_title)\n",
    "\n",
    "                for attacker in range(num_attackers):\n",
    "\n",
    "                    spamNode = Node(len(network.nodes), network_size = total_network_size)\n",
    "                    network.add_node(spamNode, fraction=0.1)\n",
    "\n",
    "                    # Bootstrap the new spam node once it enters the network\n",
    "                    bootstrap_ID = np.random.choice(range(num_nodes), size=1)\n",
    "                    bootstrap_node = network.nodes[bootstrap_ID].values[0]\n",
    "\n",
    "                    # Give it a sampling of clicklog items to gossip\n",
    "                    clicklog_sample = np.random.choice(bootstrap_node.clicklog.index.values, size=(int)(np.ceil(bootstrap_rate * len(bootstrap_node.clicklog))), replace=False)\n",
    "                    bootstrap_clicklog = bootstrap_node.clicklog.loc[clicklog_sample]\n",
    "\n",
    "                    # Make the bootstrap node aware of the new spam node\n",
    "                    bootstrap_node.gossip(spamNode, bootstrap_clicklog, sample_rate = 1.0, update=True, propagate=False)\n",
    "                    \n",
    "                    \n",
    "#                     bootstrap_node.update_known_nodes([spamNode.nodeID])\n",
    "#                     spamNode.update_clicklog(bootstrap_clicklog, single_item=False, update=False, propagate=False)\n",
    "\n",
    "                    if(PRINT_DEBUG): \n",
    "                        print('bootstrap_node ' + str(bootstrap_node.nodeID) + ' bootstrapping spamNode ' + str(spamNode.nodeID) + ' with ' + str(len(bootstrap_clicklog)) + ' clicklog items')\n",
    "                        print('Spam node ' + str(spamNode.nodeID) + ' should be aware of ' + str(len(bootstrap_clicklog.nodeID.unique())) + ' nodes. (' + str(sorted(bootstrap_clicklog.nodeID.unique())) + ')')\n",
    "                        print('Spam node ' + str(spamNode.nodeID) + ' is actually aware of ' + str(len(spamNode.clicklog.nodeID.unique())) + ' nodes. (' + str(sorted(spamNode.clicklog.nodeID.unique())) + ')')\n",
    "\n",
    "                print('Bootstrap over')\n",
    "\n",
    "\n",
    "\n",
    "            # Nodes join at 25% and attack later (at 50% or 75% or whatever)\n",
    "            if (tick == attack_time):\n",
    "\n",
    "                print('ATTACK TIME:', datetime.datetime.now())\n",
    "\n",
    "                # attackers are non-benign nodes in network\n",
    "                attackers = network.nodes.iloc[num_nodes:]\n",
    "\n",
    "                for spamNode in attackers:\n",
    "                    \n",
    "                    if (PRINT_DEBUG):\n",
    "                        print('Node', spamNode.nodeID, 'performing', num_spam, 'queries.')\n",
    "                    \n",
    "                    # Number of spam queries varies depending on simulation type\n",
    "                    for spam in range(num_spam):\n",
    "\n",
    "                        # Define a random tag to search the network for\n",
    "                        random_tag = str(tags.iloc[np.random.randint(0, len(tags))])\n",
    "\n",
    "                        # casts everything to alphanumeric + spaces\n",
    "                        random_tag = re.sub(r'\\W+ ', '', random_tag)\n",
    "\n",
    "                        # spam node performs a search just like all other nodes\n",
    "                        search_results, seen_before = search_ranked_v2(random_tag, spamNode, network, isolation)\n",
    "\n",
    "                        # spam node chooses a random result or the bottom result\n",
    "                        if (clicklog_inflation):\n",
    "                            clicked_item = choose_random(search_results, seen_before, random_tag, spamNode, network)\n",
    "                        else:\n",
    "                            clicked_item = choose_bottom(search_results, seen_before, random_tag, spamNode, network)\n",
    "\n",
    "\n",
    "                    # Manually update known nodes before gossiping   \n",
    "                    spamNode.update_known_nodes(spamNode.clicklog.nodeID.unique())\n",
    "                    if (TEST):\n",
    "                        print(datetime.datetime.now(), '- Updating known nodes')\n",
    "\n",
    "                    \n",
    "                    # Sybil nodes can only attack if its a push scheme, otherwise they must wait to be called upon\n",
    "                    if (push):\n",
    "\n",
    "                        # New spamNodes can only gossip to nodes they were bootstrapped\n",
    "                        options = [x for x in spamNode.known_nodes.index.values if x != spamNode.nodeID]\n",
    "\n",
    "                        # Pick 2 targets for the push-pull experiment\n",
    "                        gossip_targets = network.nodes[np.random.choice(options, size=1 if len(options) < 2 else 2, replace=False)]\n",
    "                        \n",
    "                        if(PRINT_DEBUG): \n",
    "                            print('Sybil Node ' + str(spamNode.nodeID) + ' gossiping to', len(gossip_targets), 'nodes.')\n",
    "\n",
    "                        to_gossip = spamNode.clicklog\n",
    "                        \n",
    "                        \n",
    "                        for target in gossip_targets:\n",
    "                            gossip_message_size.append(to_gossip.memory_usage().sum())\n",
    "                            spamNode.gossip(network.nodes[target.nodeID], to_gossip, sample_rate = 1.0, update=True, propagate=False)\n",
    "                            spamNode.known_nodes['gossip_progress'].iloc[target.nodeID] = to_gossip.tail(1).index.values[0]\n",
    "                            \n",
    "\n",
    "                    # If it's not a push scheme, they can request gossip from other nodes to make other nodes aware of them\n",
    "                    else:\n",
    "                        \n",
    "                        # New spamNodes can only gossip to nodes they were bootstrapped\n",
    "                        options = [x for x in spamNode.known_nodes.index.values if x != spamNode.nodeID]\n",
    "\n",
    "                        # only 1 target at a time\n",
    "                        gossip_targets = network.nodes[np.random.choice(options, size=1, replace=False)]\n",
    "\n",
    "                        # request updates from all nodes\n",
    "#                         gossip_targets = network.nodes[options]\n",
    "                        \n",
    "\n",
    "                \n",
    "                        for target in gossip_targets:\n",
    "                            \n",
    "                            to_gossip = target.clicklog\n",
    "                            gossip_message_size.append(to_gossip.memory_usage().sum())\n",
    "                            \n",
    "                            target.gossip(spamNode, to_gossip, sample_rate = 1.0, update=True, propagate=False)\n",
    "                            target.known_nodes['gossip_progress'].iloc[spamNode.nodeID] = to_gossip.tail(1).index.values[0]\n",
    "                            target.update_known_nodes([spamNode.nodeID])\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    \n",
    "\n",
    "            ######################## Average gossip message size:\n",
    "            if (tick % MEASURE_RATE == 0) and (tick != 0):\n",
    "                avg_gossip_message_size.append(np.mean(gossip_message_size))\n",
    "                gossip_message_size = []\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "################################################ MEASUREMENTS ######################################################\n",
    "\n",
    "\n",
    "\n",
    "            # every MEASURE_RATE we evaluate the simulation, including at the very beginning\n",
    "            if (tick % MEASURE_RATE == 0) and (MEASURING) and (tick != 0):\n",
    "\n",
    "\n",
    "                previous_measure_time = last_measure_time\n",
    "                print(measure_start)\n",
    "                print(which_sim)\n",
    "\n",
    "                clicklog_sizes = []\n",
    "\n",
    "                for n in network.nodes:\n",
    "                    clicklog_sizes.append(len(n.clicklog))\n",
    "\n",
    "                # Keep track of the round, the median, and mean of the size of the clicklog\n",
    "                log_size.append([tick, np.median(clicklog_sizes), np.mean(clicklog_sizes)])\n",
    "\n",
    "\n",
    "\n",
    "                # Only measure metrics from benign nodes (to get a pure global snapshot)\n",
    "                benign_clicklog = network.clicklog.iloc[np.where(network.clicklog.nodeID < num_nodes)]\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ######################## Average gossip message size:\n",
    "                avg_gossip_message_size.append(np.mean(gossip_message_size))\n",
    "                gossip_message_size = []\n",
    "                \n",
    "                \n",
    "                ######################## \n",
    "\n",
    "                ########################\n",
    "                # Keep track of the most popular song per tag\n",
    "                print('Checking nodes containing most popular song')\n",
    "                print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "                best_song = []\n",
    "                popularity = []\n",
    "\n",
    "\n",
    "                for tag in tags:\n",
    "                    # Most popular song by amount of clicks (global) PER TAG\n",
    "                    most_pop_song = benign_clicklog.iloc[np.where(benign_clicklog.query_term == tag)].groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(1).item_clicked.values[0]\n",
    "                    best_song.append(most_pop_song)\n",
    "\n",
    "                tag_popularity['most_popular_song_round_' + str((int)(tick / MEASURE_RATE))] = best_song\n",
    "\n",
    "\n",
    "                ##########################\n",
    "                # Keep track of the nodes having seen the most popular song per tag\n",
    "                for pop_song_id in tag_popularity['most_popular_song_round_' + str((int)(tick / MEASURE_RATE))].values:\n",
    "\n",
    "                    present = 0\n",
    "\n",
    "                    # Only looking at benevolent nodes\n",
    "                    for node in network.nodes[:num_nodes]:\n",
    "                        if pop_song_id in node.library.index.values:\n",
    "                            present = present + 1\n",
    "\n",
    "                    popularity.append(present)\n",
    "\n",
    "\n",
    "                # The median number of nodes containing the most popular songs\n",
    "                # We want to see how this grows as the number of queries grows\n",
    "                medians.append(np.median(popularity) / num_nodes)\n",
    "\n",
    "                print('Adding to Tag Popularity Dataframe')\n",
    "                print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "                tag_popularity['num_nodes_containing_round_' + str((int)(tick / MEASURE_RATE))] = popularity\n",
    "\n",
    "\n",
    "\n",
    "                ###############################\n",
    "                # GLOBAL:\n",
    "                # Change in average and median distance between true rankings and local rankings\n",
    "                #####\n",
    "\n",
    "\n",
    "                print('Calculating best global ranking for each term')\n",
    "                print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "\n",
    "                # The pairwise Jaccard similarity for all benevolent nodes (0:num_nodes)\n",
    "                global_node_distances = pd.DataFrame([[jaccard_similarity(node, network.nodes[x]) for x in range(0, num_nodes)] for node in network.nodes])\n",
    "\n",
    "\n",
    "                # get the best rankings for each term based on the number of clicks\n",
    "                best_rankings = pd.DataFrame(columns=tags)\n",
    "\n",
    "\n",
    "                for tag in tags.values:\n",
    "\n",
    "                    # All songs containing the tag in its metadata\n",
    "                    all_songs_with_tag = df_lib.iloc[np.where([tag in str(extract_tags(x)) for x in df_lib.tags.values])]\n",
    "\n",
    "                    # All global clicklog results for this tag\n",
    "                    tag_results = benign_clicklog.iloc[np.where(benign_clicklog['query_term'] == tag)].dropna(axis=0, inplace=False)\n",
    "\n",
    "                    # Sorted tag results by number of clicks\n",
    "                    ranking = tag_results.groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "\n",
    "                    # All songs with the tag not in the clicklog yet\n",
    "                    other_songs = [other for other in all_songs_with_tag.index.values if other not in ranking.item_clicked.values]\n",
    "\n",
    "                    # \"Proper\" ranking based on number of clicks across all nodes in the network (including zero clicks)\n",
    "                    proper_ranking = ranking.item_clicked.append(pd.Series(other_songs), ignore_index=True)\n",
    "\n",
    "                    # Dataset containing all global rankings for this tag\n",
    "                    best_rankings[tag] = [proper_ranking.values]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #########################\n",
    "                # LOCAL:\n",
    "                # Get the local rankings for all nodes and terms\n",
    "\n",
    "                print('Calculating the best local ranking for all terms at each node')\n",
    "                print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "                node_rankings_per_tag = pd.DataFrame(columns=tags)\n",
    "\n",
    "                # for all nodes (including malicious nodes)\n",
    "                for local_node in network.nodes:\n",
    "\n",
    "                    node_best_rankings = pd.DataFrame(columns=tags)\n",
    "\n",
    "                    # Same as global, but only on local clicklog data\n",
    "                    for term in tags.values:\n",
    "\n",
    "                        # All songs containing the tag in its metadata\n",
    "                        node_all_songs_with_term = df_lib.iloc[np.where([term in str(extract_tags(x)) for x in df_lib.tags.values])]\n",
    "\n",
    "                        # All local clicklog results for this tag\n",
    "                        node_term_results = local_node.clicklog.iloc[np.where(local_node.clicklog['query_term'] == term)].dropna(axis=0, inplace=False)\n",
    "\n",
    "                        # Ranking for this tag according to this node's clicklog\n",
    "                        node_ranking = node_term_results.groupby('item_clicked')['item_clicked'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "\n",
    "                        # Songs in the node's library that have not been searched/clicked on yet\n",
    "                        node_other_songs = [local for local in node_all_songs_with_term.index.values if local not in node_ranking.item_clicked.values]\n",
    "\n",
    "                        # \"Proper\" ranking based on local data\n",
    "                        node_proper_ranking = node_ranking.item_clicked.append(pd.Series(node_other_songs), ignore_index=True)\n",
    "\n",
    "                        # Dataset containing the most recent ranking per tag at this time\n",
    "                        node_best_rankings[term] = pd.Series([node_proper_ranking.values])\n",
    "\n",
    "                    # Keep track of all best rankings per tag, per node\n",
    "                    node_rankings_per_tag = pd.concat([node_rankings_per_tag, node_best_rankings], ignore_index=True)\n",
    "\n",
    "\n",
    "                # Get the total distances between the optimal ranking and local ranking for all terms\n",
    "                print('Calculating total distances between optimal ranking and local ranking for all terms')\n",
    "                print(' Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "                # The distance between the best ranking for TERM and node's ranking for TERM, for all nodes, for all terms (including malicious nodes)\n",
    "                distances_term = pd.Series([[list_distance(best_rankings[term][0], node_rankings_per_tag[term].iloc[y]) for y in range(len(network.nodes))] for term in tags])\n",
    "\n",
    "                # Distance between best ranking for each term and the node's ranking for the term, across all terms, for each node (including malicious nodes)\n",
    "                distances_node = pd.Series([[list_distance(best_rankings[term][0], node_rankings_per_tag[term].iloc[y]) for term in tags] for y in range(len(network.nodes))])\n",
    "\n",
    "\n",
    "                node_distances_over_time.append(distances_node.values)\n",
    "                term_distances_over_time.append(distances_term.values)\n",
    "\n",
    "\n",
    "\n",
    "                ################## Look for infected nodes ##################\n",
    "                if(not baseline):\n",
    "                    for node in network.nodes:\n",
    "                        if np.max(node.clicklog.nodeID) >= num_nodes:\n",
    "\n",
    "                            # \"Round\" meaning each measurement round\n",
    "                            node.round_infected = np.min([node.round_infected, (int)(tick / MEASURE_RATE)])\n",
    "\n",
    "\n",
    "                measure_end = datetime.datetime.now()\n",
    "                measure_time = measure_end - measure_start\n",
    "                measurement_times.append(measure_time)\n",
    "\n",
    "\n",
    "                if (TEST):\n",
    "\n",
    "                    print('--------------------------------------------------------')\n",
    "                    print('Time Elapsed:', datetime.datetime.now() - start_time)\n",
    "                    print('Measurement round time:' + str(measure_time))\n",
    "                    print('Measurement time so far:' + str(np.sum(measurement_times)))\n",
    "                    if (measure_time > last_measure_time):\n",
    "                        print('This round was SLOWER than the last')\n",
    "                    else:\n",
    "                        print('This round was FASTER than the last')\n",
    "                    if ((measure_time - last_measure_time) > (last_measure_time - previous_measure_time)):\n",
    "                        print('The rate of change is SLOWING DOWN')\n",
    "                        print('Difference:', (measure_time - last_measure_time) - (last_measure_time - previous_measure_time))\n",
    "                    else:\n",
    "                        print('The rate of change is SPEEDING UP')\n",
    "                        print('Difference:', (last_measure_time - previous_measure_time) - (measure_time - last_measure_time))\n",
    "                    print('--------------------------------------------------------')\n",
    "\n",
    "                last_measure_time = measure_time\n",
    "\n",
    "\n",
    "        print(\"Finished at:\", datetime.datetime.now())\n",
    "        print('Total duration:', datetime.datetime.now() - sim_start_time)\n",
    "        print('Total time spent during measurements:', np.sum(measurement_times))\n",
    "\n",
    "\n",
    "\n",
    "        if (AUTOSAVE):\n",
    "            \n",
    "            avg_gossip_message_size = pd.Series(avg_gossip_message_size)\n",
    "            avg_gossip_message_size.to_csv(datasets_loc + timestamp + F + model + size + 'avg_gossip_message_size.csv')\n",
    "            \n",
    "            # Smaller test sims go elsewhere\n",
    "            if (num_nodes < 100):\n",
    "                                \n",
    "                now = str(datetime.datetime.now())\n",
    "                timestamp = now[:10] + '_' + now[11:13] + 'h' + now[14:16] + 'm'\n",
    "                size = '_' + str(num_nodes) + '_'\n",
    "                DOT = pd.Series(node_distances_over_time)\n",
    "                plot_mean_distances = pd.Series([[np.mean(x) for x in DOT[y]] for y in DOT.index])\n",
    "                plot_med_distances = pd.Series([[np.median(x) for x in DOT[y]] for y in DOT.index])\n",
    "                df_distances = pd.DataFrame([node for node in plot_mean_distances])\n",
    "                df_med_distances = pd.DataFrame([node for node in plot_med_distances])\n",
    "                df_clicklog_medians = pd.DataFrame(medians)\n",
    "                round_infected = pd.Series([x.round_infected for x in network.nodes])\n",
    "\n",
    "\n",
    "                round_infected.to_csv('~/Documents/Delft/Thesis/LTR/Results/TEST/' + timestamp + F + model + size + 'round_infected.csv')\n",
    "                DOT.to_csv('~/Documents/Delft/Thesis/LTR/Results/TEST/' + timestamp + F + model + size + 'DOT.csv')\n",
    "                df_clicklog_medians.to_csv('~/Documents/Delft/Thesis/LTR/Results/TEST/' + timestamp + F + model + size + 'df_clicklog_medians.csv')\n",
    "                df_distances.to_csv('~/Documents/Delft/Thesis/LTR/Results/TEST/' + timestamp + F + model + size + 'df_distances.csv')\n",
    "                df_med_distances.to_csv('~/Documents/Delft/Thesis/LTR/Results/TEST/' + timestamp + F + model + size + 'df_med_distances.csv')\n",
    "                tag_popularity.to_csv('~/Documents/Delft/Thesis/LTR/Results/TEST/' + timestamp + F + model + size + 'tag_popularity.csv')\n",
    "                \n",
    "\n",
    "            # Full FINAL sims go here\n",
    "            else:\n",
    "                \n",
    "                now = str(datetime.datetime.now())\n",
    "                timestamp = now[:10] + '_' + now[11:13] + 'h' + now[14:16] + 'm'\n",
    "                size = '_' + str(num_nodes) + '_'\n",
    "                DOT = pd.Series(node_distances_over_time)\n",
    "                plot_mean_distances = pd.Series([[np.mean(x) for x in DOT[y]] for y in DOT.index])\n",
    "                plot_med_distances = pd.Series([[np.median(x) for x in DOT[y]] for y in DOT.index])\n",
    "                df_distances = pd.DataFrame([node for node in plot_mean_distances])\n",
    "                df_med_distances = pd.DataFrame([node for node in plot_med_distances])\n",
    "                df_clicklog_medians = pd.DataFrame(medians)\n",
    "                round_infected = pd.Series([x.round_infected for x in network.nodes])\n",
    "\n",
    "\n",
    "                round_infected.to_csv(datasets_loc + timestamp + F + model + size + 'round_infected.csv')\n",
    "                DOT.to_csv(datasets_loc + timestamp + F + model + size + 'DOT.csv')\n",
    "                df_clicklog_medians.to_csv(datasets_loc + timestamp + F + model + size + 'df_clicklog_medians.csv')\n",
    "                df_distances.to_csv(datasets_loc + timestamp + F + model + size + 'df_distances.csv')\n",
    "                df_med_distances.to_csv(datasets_loc + timestamp + F + model + size + 'df_med_distances.csv')\n",
    "                tag_popularity.to_csv(datasets_loc + timestamp + F + model + size + 'tag_popularity.csv')\n",
    "            \n",
    "\n",
    "            \n",
    "                \n",
    "    \n",
    "allDone()\n",
    "            \n",
    "\n",
    "total_finish = datetime.datetime.now()\n",
    "\n",
    "print('***************************************************')\n",
    "print('Total simulation time:', total_finish - total_start)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742788c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c578a3",
   "metadata": {},
   "source": [
    "# Immediate Visualizations\n",
    "\n",
    "These are not loaded from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOT = pd.Series(node_distances_over_time)\n",
    "plot_mean_distances = pd.Series([[np.mean(x) for x in DOT[y]] for y in DOT.index])\n",
    "plot_med_distances = pd.Series([[np.median(x) for x in DOT[y]] for y in DOT.index])\n",
    "df_distances = pd.DataFrame([node for node in plot_mean_distances])\n",
    "round_infected = pd.Series([x.round_infected for x in network.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506671a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 20,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Get all values to plot in the scatter\n",
    "to_plot = []\n",
    "for i in df_distances.index:\n",
    "    for j in df_distances.iloc[i]:\n",
    "        to_plot.append([i, j])\n",
    "        \n",
    "        \n",
    "indices = [to_plot[i][0] for i in range(len(to_plot))]\n",
    "values = [to_plot[i][1] for i in range(len(to_plot))]\n",
    "\n",
    "df_distances_infected = df_distances.copy()\n",
    "\n",
    "# Find out which values are of infected nodes\n",
    "for i in df_distances_infected.index:\n",
    "    for j in df_distances_infected.columns:\n",
    "        if round_infected.iloc[j] == i:\n",
    "            df_distances_infected.iloc[i][j] = -df_distances_infected.iloc[i][j]\n",
    "        \n",
    "        if df_distances_infected.iloc[i-1][j] < 0:\n",
    "            df_distances_infected.iloc[i][j] = -df_distances_infected.iloc[i][j]\n",
    "\n",
    "to_plot_green = []\n",
    "\n",
    "# Create the list of indices and values to plot (since plt.scatter sucks)\n",
    "for i in df_distances_infected.index:\n",
    "    for j in df_distances_infected.columns:\n",
    "        if (df_distances_infected.iloc[i][j] > 0):\n",
    "            to_plot_green.append([i, None])\n",
    "        elif (df_distances_infected.iloc[i][j] < 0):\n",
    "            to_plot_green.append([i, -df_distances_infected.iloc[i][j]])\n",
    "        else:\n",
    "            to_plot_green.append([i, None])\n",
    "            \n",
    "indices_green = [to_plot_green[i][0] for i in range(len(to_plot_green))]\n",
    "values_green = [to_plot_green[i][1] for i in range(len(to_plot_green))]\n",
    "\n",
    "\n",
    "to_plot_red = []\n",
    "\n",
    "df_distances_malicious = df_distances.copy()\n",
    "### Find the malicious modes and make them red\n",
    "for i in df_distances_malicious.index:\n",
    "    for j in df_distances_malicious.columns:\n",
    "        if (j >= num_nodes) and (df_distances_malicious.iloc[i][j] > 0):\n",
    "            to_plot_red.append([i, df_distances_malicious.iloc[i][j]])\n",
    "        elif (j >= num_nodes) and (df_distances_malicious.iloc[i][j] < 0):\n",
    "            to_plot_red.append([i, -df_distances_malicious.iloc[i][j]])\n",
    "        else:\n",
    "            to_plot_red.append([i, None])\n",
    "\n",
    "            \n",
    "indices_red = [to_plot_red[i][0] for i in range(len(to_plot_red))]\n",
    "values_red = [to_plot_red[i][1] for i in range(len(to_plot_red))]\n",
    "\n",
    "\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "\n",
    "\n",
    "plt.title('Mean Node Distance: ' + str(model_title) + ' (' + str(F_title) + ') ', fontdict=font)\n",
    "\n",
    "plt.xlabel('Evaluation Round', fontdict=font)\n",
    "plt.ylabel('Mean Distance', fontdict=font)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "bootstrap_loc = (int)(join_time / MEASURE_RATE)\n",
    "attack_loc = (int)(attack_time / MEASURE_RATE)\n",
    "\n",
    "\n",
    "\n",
    "avg = pd.Series([np.mean(df_distances.iloc[x]) for x in df_distances.index])\n",
    "median = pd.Series([np.median(df_distances.iloc[x]) for x in df_distances.index])\n",
    "\n",
    "\n",
    "plt.scatter(indices, values, color='b', label='Benign Nodes')\n",
    "\n",
    "if any(x is not None for x in values_green):\n",
    "    plt.scatter(indices_green, values_green, color='g', label='Infected Nodes')\n",
    "if any(x is not None for x in values_red):\n",
    "    plt.scatter(indices_red, values_red, color='r', label='Sybil Nodes')\n",
    "    plt.axvline(x = bootstrap_loc, color = 'g', alpha=0.75, label = 'Sybil Bootstrap')\n",
    "    plt.axvline(x = attack_loc, color = 'r', alpha=0.75, label = 'Time of Attack')\n",
    "    \n",
    "# plt.plot(avg, color='black', label='Overall Mean Distance')\n",
    "\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "plt.legend(loc='upper right', prop={'size': 16})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce5e59",
   "metadata": {},
   "source": [
    "# Figures\n",
    "\n",
    "Load the figures from past experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0df2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_title = '2023-01-21_10h05m_F0__push__100_'\n",
    "df_distances = pd.read_csv(datasets_loc + sim_title + 'df_distances.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "round_infected = pd.read_csv(datasets_loc + sim_title + 'round_infected.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "\n",
    "# df_distances = pd.read_csv(datasets_loc + sim_title + 'df_distances.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "df_distances.columns = [(int)(x) for x in df_distances.columns]\n",
    "# round_infected = pd.read_csv(datasets_loc + sim_title + 'round_infected.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "round_infected.columns = [(int)(x) for x in round_infected.columns]\n",
    "\n",
    "\n",
    "if 'targ' in sim_title:\n",
    "    plot_model_title = ' Targeted Sybil Attack '\n",
    "if 'infl' in sim_title:\n",
    "    plot_model_title = ' Clicklog Inflation Attack '\n",
    "if 'base' in sim_title:\n",
    "    plot_model_title = ' Baseline '\n",
    "if 'push' in sim_title:\n",
    "    plot_model_title = ' Push vs. Pull '\n",
    "if 'epic' in sim_title:\n",
    "    plot_model_title = ' Epic Sybil Attack '\n",
    "\n",
    "if 'F0' in sim_title:\n",
    "    plot_F_title = ' (F = 0) '\n",
    "if 'F1' in sim_title:\n",
    "    plot_F_title = ' (F = 1) '\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd143cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 36,\n",
    "        }\n",
    "\n",
    "# Get all values to plot in the scatter\n",
    "to_plot = []\n",
    "for i in df_distances.index:\n",
    "    for j in df_distances.iloc[i]:\n",
    "        to_plot.append([i, j])\n",
    "        \n",
    "        \n",
    "indices = [to_plot[i][0] for i in range(len(to_plot))]\n",
    "values = [to_plot[i][1] for i in range(len(to_plot))]\n",
    "\n",
    "\n",
    "\n",
    "df_distances_infected = df_distances.copy()\n",
    "# df_distances_infected = df_distances[:num_nodes].copy()\n",
    "\n",
    "\n",
    "# Find out which values are of infected nodes\n",
    "for i in df_distances_infected.index:\n",
    "    for j in df_distances_infected.columns[:-2]:\n",
    "        if round_infected.iloc[j].values[0] == i:\n",
    "            df_distances_infected.iloc[i][j] = -df_distances_infected.iloc[i][j]\n",
    "        \n",
    "        if df_distances_infected.iloc[i-1][j] < 0:\n",
    "            df_distances_infected.iloc[i][j] = -df_distances_infected.iloc[i][j]\n",
    "\n",
    "to_plot_green = []\n",
    "\n",
    "# Create the list of indices and values to plot (since plt.scatter sucks)\n",
    "for i in df_distances_infected.index:\n",
    "    for j in df_distances_infected.columns:\n",
    "        if (df_distances_infected.iloc[i][j] > 0):\n",
    "            to_plot_green.append([i, None])\n",
    "        elif (df_distances_infected.iloc[i][j] < 0):\n",
    "            to_plot_green.append([i, -df_distances_infected.iloc[i][j]])\n",
    "        else:\n",
    "            to_plot_green.append([i, None])\n",
    "            \n",
    "indices_green = [to_plot_green[i][0] for i in range(len(to_plot_green))]\n",
    "values_green = [to_plot_green[i][1] for i in range(len(to_plot_green))]\n",
    "\n",
    "\n",
    "to_plot_red = []\n",
    "\n",
    "df_distances_malicious = df_distances.copy()\n",
    "\n",
    "### Find the malicious modes and make them red\n",
    "for i in df_distances_malicious.index:\n",
    "    for j in df_distances_malicious[num_nodes:].columns:\n",
    "        if (j >= num_nodes) and (df_distances_malicious.iloc[i][j] > 0):\n",
    "            to_plot_red.append([i, df_distances_malicious.iloc[i][j]])\n",
    "        elif (j >= num_nodes) and (df_distances_malicious.iloc[i][j] < 0):\n",
    "            to_plot_red.append([i, -df_distances_malicious.iloc[i][j]])\n",
    "        else:\n",
    "            to_plot_red.append([i, None])\n",
    "\n",
    "            \n",
    "indices_red = [to_plot_red[i][0] for i in range(len(to_plot_red))]\n",
    "values_red = [to_plot_red[i][1] for i in range(len(to_plot_red))]\n",
    "\n",
    "\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "\n",
    "\n",
    "plt.title('Mean Node Distance:' + plot_model_title + plot_F_title + '(TEMP)', fontdict=font)\n",
    "plt.title('Mean Node Distance:' + plot_model_title + plot_F_title, fontdict=font)\n",
    "\n",
    "# plt.title(str(sim_title) + ': Mean Node Distance', fontdict=font)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Evaluation Round', fontdict=font)\n",
    "plt.ylabel('Mean Distance', fontdict=font)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "bootstrap_loc = 25\n",
    "attack_loc = 50\n",
    "\n",
    "\n",
    "\n",
    "avg = pd.Series([np.mean(df_distances.iloc[x]) for x in df_distances.index])\n",
    "median = pd.Series([np.median(df_distances.iloc[x]) for x in df_distances.index])\n",
    "\n",
    "\n",
    "plt.scatter(indices, values, color='b', label='Benign Nodes', s=75)\n",
    "\n",
    "if any(x is not None for x in values_red):\n",
    "    plt.scatter(indices_red, values_red, color='r', label='Sybil Nodes', s=100)\n",
    "    plt.scatter(indices_green, values_green, color='g', label='Infected Nodes', s=50)\n",
    "    \n",
    "    plt.axvline(x = bootstrap_loc, color = 'g', alpha=0.75, label = 'Sybil Bootstrap')\n",
    "    plt.axvline(x = attack_loc, color = 'r', alpha=0.75, label = 'Time of Attack')\n",
    "    \n",
    "# if any(x is not None for x in values_green):\n",
    "#     plt.scatter(indices_green, values_green, color='g', label='Infected Nodes', s=75)\n",
    "\n",
    "    \n",
    "# plt.plot(avg, color='black', label='Overall Mean Distance')\n",
    "\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(labeltop=False, labelright=True)\n",
    "\n",
    "lgnd = plt.legend(loc=\"upper right\", scatterpoints=1, fontsize=20)\n",
    "\n",
    "# plt.legend(loc='upper right', prop={'size': 20})\n",
    "\n",
    "lgnd.legendHandles[0]._sizes = [100]\n",
    "\n",
    "if 'baseline' not in sim_title:\n",
    "    lgnd.legendHandles[1]._sizes = [100]\n",
    "    lgnd.legendHandles[2]._sizes = [100]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8bbcb",
   "metadata": {},
   "source": [
    "# Average Gossip Message Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 30,\n",
    "        }\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "df_avg = pd.read_csv(datasets_loc + 'avg_gossip_message_size_ALL.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "\n",
    "df_avg = df_avg/1000\n",
    "\n",
    "minimum = [np.min(x) for x in [df_avg.iloc[y] for y in df_avg.index]]\n",
    "maximum = [np.max(x) for x in [df_avg.iloc[y] for y in df_avg.index]]\n",
    "\n",
    "plt.title('Mean Gossip Message Size (kB) - All Simulations', fontdict=font)\n",
    "plt.xlabel('Evaluation Round', fontdict=font)\n",
    "plt.ylabel('Size (kB)', fontdict=font)\n",
    "\n",
    "\n",
    "# plt.plot(df_avg.baseline, linewidth=2, label='Gossip Message Size (Baseline)')\n",
    "# plt.plot(df_avg.inflation, linewidth=2, label='Clicklog Inflation Attack')\n",
    "\n",
    "# plt.plot(df_avg.targ, linewidth=2, label='Targeted Sybil Attack')\n",
    "# plt.plot(df_avg.epic, linewidth=2, label='Epic Sybil Attack')\n",
    "# plt.plot(df_avg.push, linewidth=2, label='Push vs. Pull')\n",
    "\n",
    "\n",
    "# plt.plot(maximum, linewidth=2, label='Max')\n",
    "# plt.plot(minimum, linewidth=2, label='Min')\n",
    "\n",
    "\n",
    "overall_average = pd.Series([np.mean(x) for x in [df_avg.iloc[y] for y in df_avg.index]])\n",
    "plt.plot(overall_average, label='Mean Message Size (all simulations)')\n",
    "\n",
    "ax.fill_between(df_avg.index, minimum, maximum, alpha=0.2, label = 'Overall Message Size Range')\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(labeltop=False, labelright=True)\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right', prop={'size':16})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d4833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average number of nodes containing the most popular song for each query_term:\n",
    "# X is number of gossip targets\n",
    "# Y is average of (num_nodes_containing)\n",
    "\n",
    "\n",
    "tag_popularity_loaded = pd.read_csv(datasets_loc + '2023-01-22_07h40m_F1__baseline__100_tag_popularity.csv')\n",
    "\n",
    "    \n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(x = tag_popularity_loaded['tag'],\n",
    "                     y = tag_popularity_loaded['num_nodes_containing_round_100'], \n",
    "                     name = 'After 100 Rounds'))\n",
    "\n",
    "fig.add_trace(go.Bar(x = tag_popularity_loaded['tag'],\n",
    "                     y = tag_popularity_loaded['num_nodes_containing_round_50'], \n",
    "                     name = 'After 50 Rounds'))\n",
    "\n",
    "fig.add_trace(go.Bar(x = tag_popularity_loaded['tag'],\n",
    "                     y = tag_popularity_loaded['num_nodes_containing_round_25'], \n",
    "                     name = 'After 25 Rounds'))\n",
    "\n",
    "fig.add_trace(go.Bar(x = tag_popularity_loaded['tag'],\n",
    "                     y = tag_popularity_loaded['num_nodes_containing_round_5'], \n",
    "                     name = 'After 5 Rounds'))\n",
    "\n",
    "fig.add_trace(go.Bar(x = tag_popularity_loaded['tag'],\n",
    "                     y = tag_popularity_loaded['num_nodes_containing_round_1'], \n",
    "                     name = 'After 1 Round'))\n",
    "\n",
    "fig.add_trace(go.Bar(x = tag_popularity_loaded['tag'],\n",
    "                     y = tag_popularity_loaded['num_nodes_containing_round_0'], \n",
    "                     name = 'After 0 Rounds'))\n",
    "\n",
    "\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_yaxes(\n",
    "    range=(0, 100),\n",
    "    constrain='domain'\n",
    ")\n",
    "\n",
    "fig.update_layout(title = {'text':\"% Queries Containing Most Popular Song For Each Term - Baseline\", 'y':0.9, 'x':0.45, 'xanchor':'center', 'yanchor':'top'},\n",
    "                  barmode = 'overlay', \n",
    "                  title_font_size = 22,\n",
    "                  width = 1000, \n",
    "                  height = 600\n",
    "                 )\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_f0_meds = pd.read_csv(datasets_loc + '2023-01-22_23h03m_F0__baseline__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "baseline_f0_meds.columns = [(int)(x) for x in baseline_f0_meds.columns]\n",
    "\n",
    "baseline_f1_meds = pd.read_csv(datasets_loc + '2023-01-22_07h40m_F1__baseline__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "baseline_f1_meds.columns = [(int)(x) for x in baseline_f1_meds.columns]\n",
    "\n",
    "\n",
    "targ_f0_meds = pd.read_csv(datasets_loc + '2023-01-23_02h36m_F0__targ_sybil__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "targ_f0_meds.columns = [(int)(x) for x in targ_f0_meds.columns]\n",
    "\n",
    "targ_f1_meds = pd.read_csv(datasets_loc + '2023-01-23_06h37m_F1__targ_sybil__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "targ_f1_meds.columns = [(int)(x) for x in targ_f1_meds.columns]\n",
    "\n",
    "\n",
    "inflation_f0_meds = pd.read_csv(datasets_loc + '2023-01-22_19h17m_F0__clicklog_inflation__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "inflation_f0_meds.columns = [(int)(x) for x in inflation_f0_meds.columns]\n",
    "\n",
    "inflation_f1_meds = pd.read_csv(datasets_loc + '2023-01-22_20h28m_F1__clicklog_inflation__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "inflation_f1_meds.columns = [(int)(x) for x in inflation_f1_meds.columns]\n",
    "\n",
    "\n",
    "epic_f0_meds = pd.read_csv(datasets_loc + '2023-01-23_15h42m_F0__epic_sybil__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "epic_f0_meds.columns = [(int)(x) for x in epic_f0_meds.columns]\n",
    "\n",
    "epic_f1_meds = pd.read_csv(datasets_loc + '2023-01-23_14h24m_F1__epic_sybil__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "epic_f1_meds.columns = [(int)(x) for x in epic_f1_meds.columns]\n",
    "\n",
    "\n",
    "\n",
    "push_f0_meds = pd.read_csv(datasets_loc + '2023-01-21_10h05m_F0__push__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "push_f0_meds.columns = [(int)(x) for x in push_f0_meds.columns]\n",
    "\n",
    "push_f1_meds = pd.read_csv(datasets_loc + '2023-01-22_00h25m_F1__push__100_df_clicklog_medians.csv').drop('Unnamed: 0', axis=1, inplace=False)\n",
    "push_f1_meds.columns = [(int)(x) for x in push_f1_meds.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964dbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clicklog_medians = pd.DataFrame()\n",
    "\n",
    "df_clicklog_medians['baseline_f0'] = baseline_f0_meds\n",
    "df_clicklog_medians['baseline_f1'] = baseline_f1_meds\n",
    "\n",
    "df_clicklog_medians['targ_f0'] = targ_f0_meds\n",
    "df_clicklog_medians['targ_f1'] = targ_f1_meds\n",
    "\n",
    "df_clicklog_medians['inflation_f0'] = inflation_f0_meds\n",
    "df_clicklog_medians['inflation_f1'] = inflation_f1_meds\n",
    "\n",
    "df_clicklog_medians['epic_f0'] = epic_f0_meds\n",
    "df_clicklog_medians['epic_f1'] = epic_f1_meds\n",
    "\n",
    "df_clicklog_medians['push_f0'] = push_f0_meds\n",
    "df_clicklog_medians['push_f1'] = push_f1_meds\n",
    "\n",
    "\n",
    "df_clicklog_medians = df_clicklog_medians * 100\n",
    "df_clicklog_medians\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 30,\n",
    "        }\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Median % Queries Containing Top Song - All Simulations', fontdict=font)\n",
    "plt.xlabel('Evaluation Round', fontdict=font)\n",
    "plt.ylabel('% Queries', fontdict=font)\n",
    "\n",
    "\n",
    "plt.plot(df_clicklog_medians.baseline_f0, color='black', label = 'Baseline (F=0)')\n",
    "plt.plot(df_clicklog_medians.baseline_f1, color='grey', label = 'Baseline (F=1)')\n",
    "\n",
    "plt.plot(df_clicklog_medians.targ_f0, color='darkred', label = 'Targeted Sybil (F=0)')\n",
    "plt.plot(df_clicklog_medians.targ_f1, color='red', label = 'Targeted Sybil (F=1)')\n",
    "\n",
    "plt.plot(df_clicklog_medians.inflation_f0, color='darkgreen', label = 'Clicklog Inflation (F=0)')\n",
    "plt.plot(df_clicklog_medians.inflation_f1, color='lime', label = 'Clicklog Inflation (F=1)')\n",
    "\n",
    "plt.plot(df_clicklog_medians.epic_f0, color='darkorange', label = 'Epic Sybil (F=0)')\n",
    "plt.plot(df_clicklog_medians.epic_f1, color='yellow', label = 'Epic Sybil (F=1)')\n",
    "\n",
    "plt.plot(df_clicklog_medians.push_f0, color='purple', label = 'Push vs. Pull (F=0)')\n",
    "plt.plot(df_clicklog_medians.push_f1, color='violet', label = 'Push vs. Pull (F=1)')\n",
    "\n",
    "# plt.plot(overall_average, label='Overall Mean Message Size')\n",
    "\n",
    "# ax.fill_between(df_avg.index, minimum, maximum, alpha=0.2, label = 'Overall Message Size Range')\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(labeltop=False, labelright=True)\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right', prop={'size':16})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c825c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272f5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deed063c",
   "metadata": {},
   "source": [
    "---\n",
    "### Figures for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e460d",
   "metadata": {},
   "source": [
    "To find similarity score between nodes $n_i$ and $n_j$, find the following:\n",
    "\n",
    "- $\\kappa_{t} = |Q^{T}_i \\cap Q^{T}_j|  %\\hspace{10mm}  \\textit{(size of matching query terms)}$\n",
    "- $\\kappa_{m} = |C_i(Q) \\cap C_j(Q)| %\\hspace{10mm} \\textit{(size of matching query-click pairs)}$\n",
    "- $\\kappa_{u} = |C_i(Q) \\cup C_j(Q)| %\\hspace{10mm} \\textit{(union of all queries)}$\n",
    "\n",
    "\n",
    "$ \\rightarrow  S_{i}(n_j) = \\frac{\\kappa_{t} + (\\kappa_{m})^{2}}{\\kappa_{u}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299ccca",
   "metadata": {},
   "source": [
    "---\n",
    "Similarity score sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f89417",
   "metadata": {},
   "source": [
    "$$R_{i}(Q) = (\\forall k \\in K_Q), \\hspace{2mm}  \\sum_{j=0}^{N} (C_k \\cdot (S_{i}(n_j) + F))$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S_i(n_j)$ indicates the similarity score for each node pair $(n_i, n_j) \\in N$\n",
    "- $C_k$ indicates the number of clicks associated with item $k \\in K_Q$\n",
    "- $K_Q$ is the unsorted set of results for query $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3180b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
